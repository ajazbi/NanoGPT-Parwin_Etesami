{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "682defe7-0a44-4f62-8923-2cefd5d8dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import inspect\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44e2b3f8-8d02-4be8-b568-97a57dc907a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        # Ensure the data is a torch tensor\n",
    "        if not isinstance(self.data, torch.Tensor):\n",
    "            self.data = torch.tensor(self.data, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # Weight tying\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # Forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fcf98bb-74b1-4006-8268-a5f5f5f35859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.99M\n",
      "Model loaded successfully.\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(25000, 128)\n",
      "    (wpe): Embedding(64, 128)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=25000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int  # n_embd must be divisible by n_head\n",
    "    dropout: float\n",
    "    bias: bool\n",
    "\n",
    "# Load the saved configuration details\n",
    "save_dir = 'GPTper_saves'\n",
    "if not os.path.exists(save_dir):\n",
    "    raise Exception(f\"Save directory {save_dir} does not exist!\")\n",
    "\n",
    "config_path = os.path.join(save_dir, 'config_perchin.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Manually set missing keys\n",
    "config_dict['block_size'] = config_dict.get('block_size', 64)\n",
    "config_dict['vocab_size'] = config_dict.get('vocab_size', 25000)\n",
    "config_dict['n_layer'] = config_dict.get('n_layer', 4)\n",
    "config_dict['n_head'] = config_dict.get('n_head', 4)\n",
    "config_dict['n_embd'] = config_dict.get('n_embd', 128)\n",
    "config_dict['dropout'] = config_dict.get('dropout', 0.1)\n",
    "config_dict['bias'] = config_dict.get('bias', True)\n",
    "\n",
    "# Convert the config dictionary to a GPTConfig object\n",
    "config = GPTConfig(\n",
    "    block_size=config_dict['block_size'],\n",
    "    vocab_size=config_dict['vocab_size'],\n",
    "    n_layer=config_dict['n_layer'],\n",
    "    n_head=config_dict['n_head'],\n",
    "    n_embd=config_dict['n_embd'],\n",
    "    dropout=config_dict['dropout'],\n",
    "    bias=config_dict['bias']\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT(config)\n",
    "\n",
    "# Load the saved model state with map_location to map CUDA tensors to the CPU\n",
    "model_path = os.path.join(save_dir, 'final_model_perchin.pt')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd7210ff-a29e-40a0-a19e-568277516f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بنشین که با من هر نظر آید [BOM]\n",
      " گل و لاله را از باغ نوبهار آمد [EOS]\n",
      " [BOM]\n",
      " بهار است ندانم تا که گل را [BOM]\n",
      " ز گلشن بی برگ نوبهار آمد [EOS]\n",
      " [BOM]\n",
      " ز آب دیده هر شبی با صبا [BOM]\n",
      " ز آب دیده من به بوی یار آمد [EOS]\n",
      " [BOM]\n",
      " همی گویم و او با یاد جوانان [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "توانا بود هر که دانا بود [BOM]\n",
      " هم ز یک لحظه دیگر چه خواهی کرد [EOS]\n",
      " [BOM]\n",
      " مادر من چون ست درین جا که  بود [BOM]\n",
      " او را نی از من و بیگانه بود [EOS]\n",
      " [BOM]\n",
      " مادر من کرد که این درد [BOM]\n",
      " او را پارگی در میانه بود [EOS]\n",
      " [BOM]\n",
      " من پاره کردم هزار ساله [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "تو خوشگل ناز منی  [BOM]\n",
      " دست ظاهر و فتوی و خط تو محراب است [EOS]\n",
      " [BOM]\n",
      " در سر کوی تو تا پای سیه تست [BOM]\n",
      " شاه در کعبه صورت زیبایی است [EOS]\n",
      " [BOM]\n",
      " گر همه صورت تو بیند همه چین [BOM]\n",
      " هر چه صورت است به رعنای ی است [EOS]\n",
      " [BOM]\n",
      " در چمن قد تو گل و لاله زار نیست [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "دور است کاروان سحر آید [EOS]\n",
      " [BOM]\n",
      " چون بر ه شم مدام ی قانع م خراش ی [BOM]\n",
      " تا ابد اندر بهشت از و شم [EOS]\n",
      " [BOM]\n",
      " تا صباحی ست در باغ وجود ش [BOM]\n",
      " تا ابد از طبع تو باشد خزان آید [EOS]\n",
      " [BOM]\n",
      " با نسیم صبحدم از بوی تو [BOM]\n",
      " تا نسیمی از بوی تو نسیمی که بوی نشان آید [EOS]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "تیر نگاه تو در دل من  [BOM]\n",
      " چون تیر مژه از دیده پر نم شد [EOS]\n",
      " [BOM]\n",
      " بس که کرد از مژه در چشم من [BOM]\n",
      " تیر مژه از سینه جگر پر نم شد [EOS]\n",
      " [BOM]\n",
      " یارب چه جای آن بود که در و هنوز [BOM]\n",
      " این ناله به خسرو خس و خس شد [EOS]\n",
      " [BOM]\n",
      " عشق تو در دل و جان من شد [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "آخر که جانم میرود از تو بگذشت [BOM]\n",
      " عشق را چون من با هم نفرستاد [EOS]\n",
      " [BOM]\n",
      " هر چه خواهد داشت در دل آزرده او [BOM]\n",
      " تا که در دل خسته صد بار نفرستاد [EOS]\n",
      " [BOM]\n",
      " چون به گاه آمدن طلعت دیدار بد [BOM]\n",
      " عمری به روز دگر کار دگر بار نفرستاد [EOS]\n",
      " [BOM]\n",
      " گر چشم تو بر ه شم [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "از عشق تو من مردم  خون [BOM]\n",
      " می گریم اگر در دلت تریاک برآید [EOS]\n",
      " [BOM]\n",
      " بر در ت خواهم که بینم روی تو با او [BOM]\n",
      " در غیرت غلط یدم که در خاک برآید [EOS]\n",
      " [BOM]\n",
      " از دست وفا گر بر من آید [BOM]\n",
      " این گل سرخی به بوی ت که از می برآید [EOS]\n",
      " [BOM]\n",
      " خسرو کجا رسد آتش بر آتش [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "بلبلی در باغ و گل خندان [BOM]\n",
      " بر جگر آب و گل خندان در بهار آمد [EOS]\n",
      " [BOM]\n",
      " غنچه خندان شکر خنده زنان بر لب شیرین [BOM]\n",
      " شد در لب شکر خنده چو آب آمد [EOS]\n",
      " [BOM]\n",
      " در باغ رویت بدید و گفت بلبل بشنو [BOM]\n",
      " گل روی تو بر روی گل سیراب آمد [EOS]\n",
      " [BOM]\n",
      " از روی تو گر بوی گل از روی تو می  بینم [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "تو را گفتم ای دوست  [BOM]\n",
      " که  از عشق می باید مرا این درد و درمان [EOS]\n",
      " [BOM]\n",
      " به بیماری که از شوخی خوبان گشتم [BOM]\n",
      " ز سنگین حجره در فرجه افگار ان [EOS]\n",
      " [BOM]\n",
      " نهان می کرد چون جانم از عشق او [BOM]\n",
      " که در جانم در غم و دیوار هم [EOS]\n",
      " [BOM]\n",
      " همی کرد آن شوخ من در چشم بد کیش [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ای صبا توشه ای از کوی فلانی [BOM]\n",
      " سر به خاک ره تو گیرم و آنگاه درمان هم [EOS]\n",
      " [BOM]\n",
      " تو ملک سلیمان شد ی شاهد تمامی [BOM]\n",
      " تو هم به یکی ده که با من درمان هم [EOS]\n",
      " [BOM]\n",
      " از گریه چشم توام با تو شوم آگه [BOM]\n",
      " و ا فشرد ی این دیده مردم کش آن هم [EOS]\n",
      " [BOM]\n",
      " دل که به هر سو خیال تو دارد نه ای [BOM]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "# Assuming your GPT model is already loaded and available as `model`\n",
    "# Ensure the model is moved to the device once\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Function to tokenize input text and truncate to block size\n",
    "def tokenize_and_truncate_text(text, block_size):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > block_size:\n",
    "        tokens = tokens[-block_size:]  # Truncate to the last block_size tokens\n",
    "    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to decode output tokens\n",
    "def decode_output(predicted_ids):\n",
    "    predicted_tokens = predicted_ids.tolist()  # Convert tensor to list\n",
    "    predicted_text = tokenizer.decode(predicted_tokens, skip_special_tokens=False)\n",
    "    return predicted_text\n",
    "\n",
    "# Function to get model output for a single input text\n",
    "def get_model_output(model, input_text, block_size, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and truncate input text\n",
    "    input_ids = tokenize_and_truncate_text(input_text, block_size)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(input_ids)\n",
    "        logits = logits[:, -1, :] / temperature  # Apply temperature\n",
    "\n",
    "        # Top-k sampling\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        chosen_idx = torch.multinomial(probs, num_samples=1)\n",
    "        predicted_token_id = top_k_indices[0, chosen_idx[0]]\n",
    "\n",
    "        # Decode the predicted token\n",
    "        predicted_text = decode_output(predicted_token_id)\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "# Function to validate the predicted text\n",
    "def is_valid_token(token):\n",
    "    # Implement your logic to check if the token is valid or not\n",
    "    # This can include checking for known invalid sequences, punctuation, etc.\n",
    "    invalid_sequences = ['ا ی']\n",
    "    return token not in invalid_sequences\n",
    "\n",
    "# Sample input text\n",
    "input_texts = [\n",
    "    \"بنشین که با من هر نظر\",\n",
    "    \"توانا بود هر که دانا بود\",\n",
    "    \"تو خوشگل ناز منی \",\n",
    "    \"دور است کاروان سحر\",\n",
    "    \"تیر نگاه تو در دل من \",\n",
    "    \"آخر که جانم میرود\",\n",
    "    \"از عشق تو من مردم \",\n",
    "    \"بلبلی در باغ\",\n",
    "    \"تو را گفتم ای دوست \",\n",
    "    \"ای صبا توشه ای از کوی فلانی\"\n",
    "]\n",
    "\n",
    "# Define maximum BOM count\n",
    "max_bom_count = 10\n",
    "\n",
    "# Iterate over each input text\n",
    "for input_text in input_texts:\n",
    "    complete_text = input_text\n",
    "    last_few_tokens = []\n",
    "    temperature = 0.7\n",
    "    bom_count = 0\n",
    "\n",
    "    while bom_count < max_bom_count:\n",
    "        predicted_text = get_model_output(model, input_text, config.block_size, temperature=temperature, top_k=50)\n",
    "\n",
    "        # Prevent repeating sequences and filter invalid tokens\n",
    "        if predicted_text.strip() in last_few_tokens or not is_valid_token(predicted_text.strip()):\n",
    "            temperature *= 1.1  # Increase temperature to promote diversity\n",
    "            predicted_text = get_model_output(model, input_text, config.block_size, temperature=temperature, top_k=50)\n",
    "        else:\n",
    "            temperature = max(0.7, temperature * 0.95)  # Gradually decrease temperature back to a minimum\n",
    "\n",
    "        complete_text += \" \" + predicted_text.strip()  # Ensure proper spacing\n",
    "        last_few_tokens.append(predicted_text.strip())\n",
    "\n",
    "        if len(last_few_tokens) > repetition_threshold:\n",
    "            last_few_tokens.pop(0)\n",
    "\n",
    "        # Truncate the input text if it exceeds the block size\n",
    "        input_text += \" \" + predicted_text.strip()\n",
    "        input_ids = tokenize_and_truncate_text(input_text, config.block_size)\n",
    "        input_text = tokenizer.decode(input_ids[0].tolist())  # Update input_text to truncated version\n",
    "\n",
    "        # Check for special tokens like [EOS] and [BOM]\n",
    "        if '[EOS]' in predicted_text or '[BOM]' in predicted_text:\n",
    "            complete_text += '\\n'\n",
    "            bom_count += 1\n",
    "    # Print the whole output after the loop, line by line\n",
    "    lines = complete_text.split('\\n')\n",
    "    #print(f\"Predicted Text for '{input_text}':\")\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08766e-130e-4d23-84cb-37aacb44eaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
