{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7747c31e-c592-4cdc-b5ed-ab9656fe39be",
      "metadata": {
        "id": "7747c31e-c592-4cdc-b5ed-ab9656fe39be"
      },
      "source": [
        "# Small GPT, little update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9615469-6d91-40ba-a58c-066f3c7e4097",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9615469-6d91-40ba-a58c-066f3c7e4097",
        "outputId": "fb737702-3088-448c-d35f-5039e760d7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "#!pip install huggingface_hub\n",
        "#!pip install transformers\n",
        "!pip install kaggle\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb38020-c7a1-4756-bebc-a7ca896c5677",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb38020-c7a1-4756-bebc-a7ca896c5677",
        "outputId": "9b82fd11-c52c-4b8b-b24c-dd9e1799514c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. GPU is ready for use.\n",
            "Device Name: NVIDIA A100-SXM4-40GB\n",
            "Number of GPUs available: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. GPU is ready for use.\")\n",
        "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "else:\n",
        "    print(\"CUDA is not available. Running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "IsfL2pzPzfD9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9df66f6c-e289-4504-f029-f79095421011"
      },
      "id": "IsfL2pzPzfD9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-487a331a-018f-4322-bc47-daf68d407d15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-487a331a-018f-4322-bc47-daf68d407d15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ajazbi\",\"key\":\"b05383cb636da48dedb1a00d1b53c555\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "QZ_VQ5Pd1FXH"
      },
      "id": "QZ_VQ5Pd1FXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n"
      ],
      "metadata": {
        "id": "GLNWDs1s1JVD"
      },
      "id": "GLNWDs1s1JVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f88c813-2c60-434f-9dcf-33a704353a90",
      "metadata": {
        "id": "7f88c813-2c60-434f-9dcf-33a704353a90"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from collections import Counter\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import inspect\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "import threading\n",
        "import queue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682b94f6-30b4-4b89-8843-4ba1423bc159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "682b94f6-30b4-4b89-8843-4ba1423bc159",
        "outputId": "73bd9cef-4aba-4d92-a5ee-c9ff2917a9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/aminghd/large-corpus-of-farsi-poems\n",
            "Dataset URL: https://www.kaggle.com/datasets/aminghd/large-corpus-of-farsi-poems\n",
            "Dataset URL: https://www.kaggle.com/datasets/aminghd/large-corpus-of-farsi-poems\n",
            "Dataset URL: https://www.kaggle.com/datasets/aminghd/large-corpus-of-farsi-poems\n",
            "Total words: 447175\n",
            "Max line length: 14 words\n",
            "Total number of lines: 62794\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Authenticate with Kaggle\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# Define Kaggle dataset details\n",
        "dataset = 'aminghd/large-corpus-of-farsi-poems'\n",
        "file_names = ['parvin_norm.txt','gilani_norm.txt','shahriar_norm.txt','khosro_norm.txt']\n",
        "#file_names = ['parvin_norm.txt', 'eraghi_norm.txt', 'farrokhi_norm.txt',\n",
        "#              'helali_norm.txt', 'gilani_norm.txt', 'khosro_norm.txt'\n",
        "#              , 'salman_norm.txt', 'shahriar_norm.txt']  # Add more file names as needed\n",
        "\n",
        "# Initialize variables to store the word count, max line length, and number of lines\n",
        "total_words = 0\n",
        "max_line_length = 0\n",
        "num_lines = 0\n",
        "\n",
        "# Initialize an empty list to store the formatted lines\n",
        "formatted_lines = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    # Download the specific file from the Kaggle dataset\n",
        "    api.dataset_download_file(dataset, file_name)\n",
        "\n",
        "    # Check if the file is downloaded and unzipped correctly\n",
        "    if not os.path.exists(file_name):\n",
        "        os.system(f'unzip {file_name}.zip')\n",
        "\n",
        "    # Read the content of the text file\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "    # Iterate over the lines in pairs and format them with special tokens\n",
        "    for i in range(0, len(lines), 2):\n",
        "        if i + 1 < len(lines) and lines[i] and lines[i + 1]:\n",
        "            formatted_line = f\"[BOM] {lines[i]} [BOM] {lines[i+1]}[EOS]\"\n",
        "            formatted_lines.append(formatted_line)\n",
        "\n",
        "            # Update line and word counts\n",
        "            words_in_first_line = len(lines[i].split())\n",
        "            words_in_second_line = len(lines[i + 1].split())\n",
        "\n",
        "            total_words += words_in_first_line + words_in_second_line\n",
        "            max_line_length = max(max_line_length, words_in_first_line, words_in_second_line)\n",
        "            num_lines += 2  # Add 2 for each valid pair\n",
        "\n",
        "# Join the formatted lines into a single string\n",
        "sentences = \"\\n\".join(formatted_lines)\n",
        "\n",
        "# Print the entire formatted content\n",
        "# print(sentences)\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Total words: {total_words}\")\n",
        "print(f\"Max line length: {max_line_length} words\")\n",
        "print(f\"Total number of lines: {num_lines}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de8952a-3ec1-4bb7-b159-85e1ce1a6d56",
      "metadata": {
        "id": "3de8952a-3ec1-4bb7-b159-85e1ce1a6d56",
        "outputId": "11d22541-3a79-412b-91f5-0302dbc63035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 24786\n"
          ]
        }
      ],
      "source": [
        "# Function to clean and count word frequencies\n",
        "def count_word_frequencies(text):\n",
        "    # Remove special tokens\n",
        "    cleaned_text = re.sub(r'\\[BOM\\]|\\[EOS\\]', '', text)\n",
        "\n",
        "    # Split text into words\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# Count word frequencies in the formatted content\n",
        "word_frequencies = count_word_frequencies(sentences)\n",
        "\n",
        "# Get the number of unique words\n",
        "num_unique_words = len(word_frequencies)\n",
        "\n",
        "# Sort word frequencies from highest to lowest\n",
        "sorted_word_frequencies = word_frequencies.most_common()\n",
        "\n",
        "# Print the total number of unique words\n",
        "print(f'Total unique words: {num_unique_words}')\n",
        "\n",
        "# Print word frequencies from highest to lowest\n",
        "#for word, freq in sorted_word_frequencies:\n",
        "#    print(f'{word}: {freq}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21c056f-0be4-48c5-8a54-a71d11bb32dc",
      "metadata": {
        "id": "e21c056f-0be4-48c5-8a54-a71d11bb32dc",
        "outputId": "780884d9-0976-4367-af32-e7ba85b30cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "6664e2cfbe1440118cbe620f098a48b5",
            "3b84199787324c18a9ef10173c481854",
            "75e78b4ad63347d8887afd8508445180",
            "386d141d54c44bed93425df5b8bab56a",
            "db4ca1e4aa404d8c9254154b69319362",
            "ffc578bc8360411dae1a505477fa862e",
            "9fd996c3cca3405990cc251fc7b87332",
            "fd1e254232674161b8266d4fd0d16011",
            "208629c1367546d5b725a93a38bf1a3a",
            "715bdbe1a9f8482db737d0ca9539f13f",
            "4fd9810ec60f4f0ca1842572f820df71",
            "4fe67eafb2a94ffdaaa7844305f0634b",
            "5dafe1799aea4dec9008a215994c6bd5",
            "bea0ccc19c364661b1e96626b6a2f19d",
            "5d6f6a0999094d889c596256aab1f976",
            "4c0db3c4591b46a9b6af09ec8d1ac6f0",
            "42a6ad44e46147e0ad8a6680b812ef5c",
            "d4e2b2d131a94468936bcd7d03093c5e",
            "4f973329ffea4c758ad8e79aefc06e49",
            "99d3bf145f544cc8a904585fb31d2213",
            "80f684cf20c547eabefa5edb0d5b496e",
            "3cc247ba80324e539d3a1a6dc5866ae3",
            "bcc03cebf4854db9a16ff13fee026cce",
            "a1d63ec5b3c647aea453da4c37a0fc35",
            "3fb6c408f497476e8e17ea32cd878275",
            "173343cf9dfa4179b46b3af7e512f601",
            "0ee1423d1b22456199af800eb53b73f0",
            "b6248b496ca24de0a235f2c12618f907",
            "8392fc49e3ef42b9af1556284cec94fe",
            "88f65ce4e5954532abf7f7d63967acfb",
            "2581473355464798a0c34b49d13b4295",
            "16f495acabdc46cb83623b136e16de38",
            "bac3586499e949b58bb757e7b0b33f62",
            "c4089e885ff24840b4c369d12ca8f871",
            "b733b000d85a4cb9b56b92201d406e15",
            "63e703c2d4e848e78c0fa950ede6b1ae",
            "a7361a404f534e148f15632a8474c061",
            "955cfb2e343e4f4e8908c7fa4aa94902",
            "6305c9d5b8c240eaa00588e48988d2fa",
            "d6d7bd48dcd740dd8b76dc830e915186",
            "d22d6778c3a942d988f6860f94d02e51",
            "91cb2cc712d24364a20cdd76cfd97ece",
            "6e55a74807774b078d4f6d517f47aa62",
            "5b94e4b316c9421581d6df2aa1ee46cd",
            "2a5f41a528de4a249cb42379ddce014c",
            "ed86efbf9d6544fda8aa08296c53b095",
            "64d56d1a6b634542ab5774b99c27d503",
            "9cc1844ce7fe4d409abfde37be8a915e",
            "82d3e26caeb5432e97a389f6d27f1ab2",
            "8cf352a365c34febb63ec37bf70d8bdd",
            "619b5491c02a4222840032e6dc486672",
            "2d65606cd1514695a9c5b0593db6c50a",
            "d6e985ed91fe40f79d8a758c73dc344c",
            "9634ba56bf2d49c7a24fea1b297d633d",
            "8c4dc91bd4604cba8be3ea7204e1df2d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 31397\n",
            "Number of training sentences: 28257\n",
            "Number of validation sentences: 3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6664e2cfbe1440118cbe620f098a48b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fe67eafb2a94ffdaaa7844305f0634b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/537k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcc03cebf4854db9a16ff13fee026cce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4089e885ff24840b4c369d12ca8f871"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a5f41a528de4a249cb42379ddce014c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data has 528,268 tokens\n",
            "Validation data has 54,897 tokens\n"
          ]
        }
      ],
      "source": [
        "sentences=formatted_lines\n",
        "# Check the number of sentences\n",
        "num_sentences = len(sentences)\n",
        "print(f\"Number of sentences: {num_sentences}\")\n",
        "\n",
        "# Ensure there are enough sentences to split\n",
        "if num_sentences > 1:\n",
        "    # Calculate the split index\n",
        "    split_index = int(num_sentences * 0.9)\n",
        "\n",
        "    # Split data into training and validation sets (90% training, 10% validation)\n",
        "    train_sentences = sentences[:split_index]\n",
        "    val_sentences = sentences[split_index:]\n",
        "\n",
        "    # Concatenate all training and validation sentences into single texts\n",
        "    train_text = ' '.join(train_sentences)\n",
        "    val_text = ' '.join(val_sentences)\n",
        "\n",
        "    print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "    print(f\"Number of validation sentences: {len(val_sentences)}\")\n",
        "else:\n",
        "    print(\"Not enough sentences to split. Please provide more data.\")\n",
        "\n",
        "# Load a Persian-specific tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/gpt2-fa')\n",
        "\n",
        "# Encode the data using the Persian-specific tokenizer\n",
        "train_ids = tokenizer.encode(train_text, add_special_tokens=False)\n",
        "val_ids = tokenizer.encode(val_text, add_special_tokens=False)\n",
        "\n",
        "print(f\"Training data has {len(train_ids):,} tokens\")\n",
        "print(f\"Validation data has {len(val_ids):,} tokens\")\n",
        "\n",
        "# Convert to NumPy arrays and save to binary files  # <-- Changed to uint32\n",
        "train_ids = np.array(train_ids, dtype=np.uint32)\n",
        "val_ids = np.array(val_ids, dtype=np.uint32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total tokens\n",
        "total_tokens = len(train_ids) + len(val_ids)\n",
        "\n",
        "# Calculate unique tokens\n",
        "unique_tokens = len(np.unique(np.concatenate((train_ids, val_ids))))\n",
        "\n",
        "# Print total tokens and unique tokens\n",
        "print(f\"Total tokens: {total_tokens}\")\n",
        "print(f\"Total unique tokens: {unique_tokens}\")\n",
        "\n",
        "# Calculate the length of tokens in each line\n",
        "line_token_lengths = [len(tokenizer.encode(line, add_special_tokens=False)) for line in sentences]\n",
        "\n",
        "# Find the maximum token length\n",
        "max_token_length = max(line_token_lengths)\n",
        "\n",
        "# Print the maximum token length\n",
        "print(f\"Maximum token length in a line: {max_token_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC28OhS91oA8",
        "outputId": "77502e1d-d55c-4c2f-f228-f25ba42b63d1"
      },
      "id": "lC28OhS91oA8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 583165\n",
            "Total unique tokens: 11331\n",
            "Maximum token length in a line: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the vocabulary size\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(f\"Tokenizer vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeJVWU7c2rur",
        "outputId": "aa9f8819-1ebf-4851-d6e3-7eec42ece30b"
      },
      "id": "jeJVWU7c2rur",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_small_gpt_parameters(total_tokens, unique_tokens, compute_budget):\n",
        "    \"\"\"\n",
        "    Calculate optimal GPT parameters for a small model based on modified Chinchilla scaling laws.\n",
        "\n",
        "    Args:\n",
        "    - total_tokens (int): Total number of tokens in the dataset.\n",
        "    - unique_tokens (int): Total number of unique tokens (vocabulary size).\n",
        "    - compute_budget (float): Total compute budget in FLOPs.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing optimal GPT parameters.\n",
        "    \"\"\"\n",
        "    # Modified Chinchilla scaling heuristic for token-to-parameter ratio (increased for smaller models)\n",
        "    token_to_param_ratio = 40\n",
        "\n",
        "    # Estimate the optimal model size\n",
        "    optimal_model_size = math.sqrt(compute_budget / token_to_param_ratio)\n",
        "\n",
        "    # Set tighter bounds for the parameters\n",
        "    max_n_embd = 512   # Upper limit for embedding size in smaller models\n",
        "    min_n_embd = 64    # Lower limit for embedding size in smaller models\n",
        "\n",
        "    max_n_layer = 12   # Upper limit for layers in smaller models\n",
        "    min_n_layer = 2    # Lower limit for layers in smaller models\n",
        "\n",
        "    # Adjust the scaling to fit within the bounds\n",
        "    scaling_factor = 1e10  # Increased to further reduce model size\n",
        "\n",
        "    # Set the model parameters based on the optimal model size\n",
        "    n_embd = int(min_n_embd + (optimal_model_size / scaling_factor) * (max_n_embd - min_n_embd))\n",
        "    n_embd = max(min_n_embd, min(max_n_embd, n_embd))  # Ensure n_embd is within bounds\n",
        "\n",
        "    n_layer = int(min_n_layer + (optimal_model_size / scaling_factor) * (max_n_layer - min_n_layer))\n",
        "    n_layer = max(min_n_layer, min(max_n_layer, n_layer))  # Ensure n_layer is within bounds\n",
        "\n",
        "    # Ensure n_embd is a multiple of n_head\n",
        "    n_head = max(2, n_embd // 64)  # Ensure at least 1 head and n_embd is divisible by n_head\n",
        "    n_embd = (n_embd // n_head) * n_head\n",
        "\n",
        "    # fetch vocab_size for tokenizer\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Calculate approximate number of parameters\n",
        "    num_params = n_embd * n_layer * (4 * n_embd + vocab_size) / 1e6  # in millions\n",
        "\n",
        "    # Return the calculated parameters\n",
        "    return {\n",
        "        \"n_embd\": n_embd,\n",
        "        \"n_head\": n_head,\n",
        "        \"n_layer\": n_layer,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"block_size\": 32,  # Reduced context size for smaller models\n",
        "        \"approx_params_millions\": round(num_params, 2)\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "compute_budget = 1e18  # Reduced compute budget for smaller models\n",
        "\n",
        "gpt_parameters = calculate_small_gpt_parameters(total_tokens, unique_tokens, compute_budget)\n",
        "print(gpt_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9l15QACzVzD",
        "outputId": "076aaa99-fb82-4581-e22a-7528e8aa000f"
      },
      "id": "t9l15QACzVzD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_embd': 70, 'n_head': 2, 'n_layer': 2, 'vocab_size': 25000, 'block_size': 32, 'approx_params_millions': 3.54}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b1adbd-d079-4e86-a2f9-e98b0dc0ca66",
      "metadata": {
        "id": "30b1adbd-d079-4e86-a2f9-e98b0dc0ca66",
        "outputId": "47fa0431-85af-4d22-b96a-147162d6d31f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Sample:\n",
            "[BOM] ای دل عبث مخور غم دنیا را[BOM] فکرت مکن نیامده فردا را[EOS][BOM] کنج قفس چو نیک بیندیشی[BOM] چون گلشن است مرغ شکیبا را[EOS][BOM] بشکاف خاک را و ببین آنگه[BOM] بی مهری زمانه رسوا را[EOS][BOM] این دشت خوابگاه شهیدانست[BOM] فرصت شمار وقت تماشا را[EOS][BOM] از عمر رفته نیز شماری کن\n",
            "\n",
            "Validation Data Sample:\n",
            "[BOM] عشق از دو صنم بود عنان تاب[BOM] چون دین ز توجه دو محراب[EOS][BOM] تا یک سر مو بود به جایت[BOM] یک مو نکشم سر از هوایت[EOS][BOM] اینجا من و دلستانم آنجاست[BOM] آنجاست دلم که جانم آنجاست[EOS][BOM] گر کرد سپهر بی طریقم[BOM] تهمت زده دگر رفیقم[EOS][BOM]\n"
          ]
        }
      ],
      "source": [
        "sample_size=64\n",
        "print(\"Training Data Sample:\")\n",
        "print(tokenizer.decode(train_ids[:sample_size]))\n",
        "\n",
        "print(\"\\nValidation Data Sample:\")\n",
        "print(tokenizer.decode(val_ids[:sample_size]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly sample starting index\n",
        "        start_idx = np.random.randint(0, len(self.data) - self.block_size - 1)\n",
        "        x = self.data[start_idx:start_idx + self.block_size]\n",
        "        y = self.data[start_idx + 1:start_idx + self.block_size + 1]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_data_loader(data, batch_size, block_size):\n",
        "    dataset = CustomDataset(data, block_size)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "batch_size = 256\n",
        "block_size = 64\n",
        "\n",
        "# Create DataLoaders for training and validation data\n",
        "train_loader = get_data_loader(train_ids, batch_size, block_size)\n",
        "val_loader = get_data_loader(val_ids, batch_size, block_size)\n"
      ],
      "metadata": {
        "id": "gZ4ZsV56mrF3"
      },
      "id": "gZ4ZsV56mrF3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_first_and_last_five(data_loader, tokenizer):\n",
        "    \"\"\"\n",
        "    Print the first and last five data points (x and y) from the DataLoader.\n",
        "\n",
        "    Args:\n",
        "    - data_loader (DataLoader): The DataLoader object.\n",
        "    - tokenizer (AutoTokenizer): The tokenizer for decoding.\n",
        "    \"\"\"\n",
        "    x_batch, y_batch = next(iter(data_loader))\n",
        "\n",
        "    def print_data_points(data, label):\n",
        "        batch_size = data.size(0)\n",
        "        seq_len = data.size(1)\n",
        "\n",
        "        print(f\"\\nFirst 5 sequences in {label}:\")\n",
        "        for i in range(min(5, batch_size)):\n",
        "            tokens = data[i].tolist()\n",
        "            decoded = tokenizer.decode(tokens)\n",
        "            print(f\"{label}[{i}]: Tokens: {tokens}, Decoded: {decoded}\")\n",
        "\n",
        "        print(f\"\\nLast 5 sequences in {label}:\")\n",
        "        for i in range(max(0, batch_size - 5), batch_size):\n",
        "            tokens = data[i].tolist()\n",
        "            decoded = tokenizer.decode(tokens)\n",
        "            print(f\"{label}[{i}]: Tokens: {tokens}, Decoded: {decoded}\")\n",
        "\n",
        "    print(\"First and Last 5 data points in x:\")\n",
        "    print_data_points(x_batch, 'x')\n",
        "\n",
        "    print(\"\\nFirst and Last 5 data points in y:\")\n",
        "    print_data_points(y_batch, 'y')\n",
        "\n",
        "\n",
        "# Print the first and last 5 sequences of both x and y for training and validation data\n",
        "print(\"Training Data:\")\n",
        "print_first_and_last_five(train_loader, tokenizer)\n",
        "\n",
        "print(\"\\nValidation Data:\")\n",
        "print_first_and_last_five(val_loader, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEbBLvIuMI21",
        "outputId": "a38870d6-01ee-408b-a450-66154bda094f"
      },
      "id": "hEbBLvIuMI21",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "First and Last 5 data points in x:\n",
            "\n",
            "First 5 sequences in x:\n",
            "x[0]: Tokens: [107, 7356, 48, 542, 7, 2298, 291, 1091, 1984, 45, 769, 80, 605, 2960, 61, 161, 213, 9, 7, 213, 562, 22812, 612, 10681, 1545, 99, 213, 7, 96, 209, 48, 73, 681, 127, 991, 15920, 46, 209, 99, 213, 9, 7, 46, 19834, 72, 90, 3229, 129, 423, 52, 836, 889, 7, 21001, 51, 46, 90, 1670, 213, 606, 128, 3611, 99, 213], Decoded: نمی مانی به کس[BOM] خسرو همان بنده ست و بس تو آنکه بودی آن نه ای[EOS][BOM] ای درد بیدرد دلم تاراج پنهان کرده ای[BOM] یا جان بهم بیرون روی کآرام در جان کرده ای[EOS][BOM] در حیرتم تا هر شبی چون خواب می آید ترا[BOM] زینسان که در هر گوشه ای صد دل پریشان کرده ای\n",
            "x[1]: Tokens: [51, 105, 1160, 120, 542, 137, 6538, 9, 7, 573, 61, 10984, 51, 6293, 19507, 7, 2910, 57, 7846, 67, 15415, 157, 9, 7, 2792, 94, 51, 2177, 50, 116, 10663, 7, 2177, 53, 95, 2171, 864, 48, 131, 9, 7, 1376, 57, 90, 51, 9212, 92, 7625, 7, 90, 116, 53, 2918, 4121, 668, 11102, 9, 7, 1091, 2879, 1669, 121, 7, 333], Decoded: که ز وزن همه کس خواهد کاست[EOS][BOM] ره آن پوی که پیدایش ازوست[BOM] لیک با اینهمه خود ناپیداست[EOS][BOM] نتوان گفت که خار از چه دمید[BOM] خار را نیز درین باغ بهاست[EOS][BOM] چرخ با هر که نشاندت بنشین[BOM] هر چه را خواجه روا دید رواست[EOS][BOM] بنده شایسته تنهایی نیست[BOM] حق\n",
            "x[2]: Tokens: [4815, 53, 2404, 998, 55, 9, 7, 113, 92, 359, 4315, 1589, 10984, 7, 197, 43, 11903, 64, 605, 103, 10184, 55, 9, 7, 21651, 56, 105, 1697, 587, 4624, 7, 161, 105, 90, 142, 1424, 9509, 55, 9, 7, 2445, 3777, 53, 116, 3418, 157, 7, 2270, 2398, 53, 116, 1237, 45, 64, 55, 9, 7, 50, 467, 2465, 80, 16159, 7, 51], Decoded: دهر را بسی پسر است[EOS][BOM] اگرت دیده ایست راهی پوی[BOM] چند خندی بر آنکه بی بصر است[EOS][BOM] نیکنامی ز نیک کاری زاد[BOM] نه ز هر نام شخص نامور است[EOS][BOM] خویشتن خواه را چه معرفتست[BOM] شاخه عجب را چه برگ و بر است[EOS][BOM] از سخن گفتن تو دانستم[BOM] که\n",
            "x[3]: Tokens: [5291, 3964, 7, 999, 3474, 1975, 78, 54, 423, 1365, 681, 1433, 9, 7, 6098, 50, 7551, 78, 7234, 123, 135, 5908, 50, 4946, 7, 128, 204, 46, 2585, 71, 2871, 209, 681, 1433, 9, 7, 197, 244, 15857, 3396, 64, 2759, 2298, 83, 11771, 7, 5093, 4670, 208, 221, 9773, 56, 999, 19022, 681, 1433, 9, 7, 5291, 54, 1086, 3492, 105, 3163], Decoded: یارب گهی[BOM] کز سیه بخت من این خواب گران بیرون رود[EOS][BOM] بگذر از بالین من کاسان شود مردن از آنک[BOM] دل چو در حسرت بود دشوار جان بیرون رود[EOS][BOM] چند بپسندی ستم برجان خسرو هم بترس[BOM] زانکه نیاد بازتیری کزکمان بیرون رود[EOS][BOM] یارب این اندیشه جانان ز جانم\n",
            "x[4]: Tokens: [46, 167, 43, 60, 7, 1173, 60, 43, 11422, 73, 57, 1594, 43, 60, 9, 7, 20803, 1604, 1534, 45, 3032, 105, 114, 7, 103, 6226, 110, 53, 1033, 242, 55, 9, 7, 333, 11164, 73, 2335, 73, 2754, 43, 60, 7, 11918, 57, 5656, 56, 11410, 43, 60, 9, 7, 54, 467, 60, 1090, 80, 1033, 254, 7, 80, 116, 3593, 19601, 56], Decoded: در راه ها[BOM] اشکها آمیختم با آه ها[EOS][BOM] بدره زر دیدم و رفتم ز دست[BOM] بی تامل روز را گفتم شب است[EOS][BOM] حق نهفتم بافتم افسانه ها[BOM] سوختم با تهمتی کاشانه ها[EOS][BOM] این سخنها بهر تو گفتم تمام[BOM] تو چه گفتی آرمیدی\n",
            "\n",
            "Last 5 sequences in x:\n",
            "x[251]: Tokens: [121, 647, 242, 464, 307, 50, 183, 364, 9, 7, 1960, 6537, 73, 213, 1359, 51, 103, 312, 73, 7, 2573, 2036, 46, 126, 78, 51, 239, 3474, 73, 9, 7, 48, 209, 7256, 1779, 128, 6376, 105, 1090, 337, 7, 51, 80, 105, 4502, 78, 45, 78, 105, 4502, 128, 6167, 9, 7, 48, 1535, 6163, 113, 52, 3301, 6376, 3822, 7, 4694], Decoded: نیست صبح شب غم کم از هزار مهم[EOS][BOM] مکن نصیحتم ای آشنا که بی خبرم[BOM] مدار آینه در پیش من که رو سیهم[EOS][BOM] به جان رسیدم ازین دل بکش ز بهر خدا[BOM] که تو ز ننگ من و من ز ننگ دل برهم[EOS][BOM] به جرم عشقم اگر می کشی بکش لیکن[BOM] نویس\n",
            "x[252]: Tokens: [157, 9, 7, 285, 1019, 157, 103, 80, 253, 90, 197, 7, 46, 45, 57, 73, 50, 8680, 252, 3042, 157, 9, 7, 98, 4674, 53, 105, 2866, 53, 92, 820, 416, 7, 9785, 335, 51, 7044, 706, 157, 9, 7, 48, 2866, 2264, 239, 74, 2298, 1845, 7, 1219, 22211, 16400, 157, 9, 7, 285, 50, 127, 5621, 4838, 126, 55, 7, 1836], Decoded: ست[EOS][BOM] مرا زندانست بی تو خانه هر چند[BOM] در و بام از خیالت پر نگارست[EOS][BOM] دو چشمم را ز کویت راتبه خاک[BOM] زیادت کن که مزد انتظارست[EOS][BOM] به کویت زرد رو شد خسرو آی[BOM] هوای نیکوان ناسازگارست[EOS][BOM] مرا از روی خوبان قبله پیش است[BOM] مسلمانان\n",
            "x[253]: Tokens: [78, 96, 2653, 7, 64, 7719, 980, 54, 3396, 691, 1984, 9, 7, 209, 48, 500, 76, 1133, 737, 7, 535, 3600, 1517, 83, 691, 1984, 9, 7, 19864, 57, 242, 4102, 9360, 7, 11227, 4524, 80, 46, 601, 691, 1984, 9, 7, 235, 105, 13178, 80, 103, 181, 55, 7, 1191, 105, 5245, 80, 14340, 55, 9, 7, 6338, 43, 58, 10948, 22274], Decoded: من یا رب[BOM] برکسی هرگز این ستم رفته ست[EOS][BOM] جان به دنبال او روان کردم[BOM] گر نیاید حیات هم رفته ست[EOS][BOM] خسروا با شب فراق بساز[BOM] کافتاب تو در عدم رفته ست[EOS][BOM] گل ز رخساره تو بی آب است[BOM] مه ز نظاره تو بیتاب است[EOS][BOM] مژه های کژ دلاویز\n",
            "x[254]: Tokens: [105, 705, 1993, 181, 9, 7, 45, 123, 1253, 2413, 14448, 7, 10508, 6148, 45, 1260, 46, 423, 9, 7, 213, 20753, 53, 73, 1391, 46, 1662, 7, 7541, 51, 1662, 53, 11611, 256, 9, 7, 252, 52, 3647, 45, 46, 3780, 13530, 185, 7, 999, 252, 5848, 80, 252, 15794, 9, 7, 769, 129, 80, 4779, 6791, 7, 3179, 256, 573, 6892, 45], Decoded: ز سنگ چشمه آب[EOS][BOM] وان مست شراب ارغوانی[BOM] مخمور فتاد و ماند در خواب[EOS][BOM] ای مرغک رام گشته در دام[BOM] برخیز که دام را گسستند[EOS][BOM] پر میزن و در سپهر بخرام[BOM] کز پر شکن تو پر شکستند[EOS][BOM] بس چون تو پرندگان گمنام[BOM] جستند ره خلاص و\n",
            "x[255]: Tokens: [51, 1144, 80, 352, 46, 5978, 52, 4161, 7, 105, 4339, 405, 91, 13217, 52, 4161, 9, 7, 1063, 105, 1479, 80, 2296, 1594, 5684, 56, 7, 51, 54, 1476, 4274, 91, 46, 5978, 52, 4161, 9, 7, 48, 2501, 2615, 128, 78, 918, 4686, 1222, 246, 7, 204, 2548, 2548, 3560, 999, 2079, 52, 4161, 9, 7, 107, 8744, 51, 128, 3249, 1984], Decoded: که بوی تواش در دماغ می افتد[BOM] ز زندگانی خویشش فراغ می افتد[EOS][BOM] شدم ز زلف تو دیوانه آه مسکینی[BOM] که این خیال کجش در دماغ می افتد[EOS][BOM] به قطره سوز دل من همی کشد زین چشم[BOM] چو شعله شعله گلی کز چراغ می افتد[EOS][BOM] نمی زید که دل سوخته ست\n",
            "\n",
            "First and Last 5 data points in y:\n",
            "\n",
            "First 5 sequences in y:\n",
            "y[0]: Tokens: [7356, 48, 542, 7, 2298, 291, 1091, 1984, 45, 769, 80, 605, 2960, 61, 161, 213, 9, 7, 213, 562, 22812, 612, 10681, 1545, 99, 213, 7, 96, 209, 48, 73, 681, 127, 991, 15920, 46, 209, 99, 213, 9, 7, 46, 19834, 72, 90, 3229, 129, 423, 52, 836, 889, 7, 21001, 51, 46, 90, 1670, 213, 606, 128, 3611, 99, 213, 9], Decoded: مانی به کس[BOM] خسرو همان بنده ست و بس تو آنکه بودی آن نه ای[EOS][BOM] ای درد بیدرد دلم تاراج پنهان کرده ای[BOM] یا جان بهم بیرون روی کآرام در جان کرده ای[EOS][BOM] در حیرتم تا هر شبی چون خواب می آید ترا[BOM] زینسان که در هر گوشه ای صد دل پریشان کرده ای[EOS]\n",
            "y[1]: Tokens: [105, 1160, 120, 542, 137, 6538, 9, 7, 573, 61, 10984, 51, 6293, 19507, 7, 2910, 57, 7846, 67, 15415, 157, 9, 7, 2792, 94, 51, 2177, 50, 116, 10663, 7, 2177, 53, 95, 2171, 864, 48, 131, 9, 7, 1376, 57, 90, 51, 9212, 92, 7625, 7, 90, 116, 53, 2918, 4121, 668, 11102, 9, 7, 1091, 2879, 1669, 121, 7, 333, 1888], Decoded: ز وزن همه کس خواهد کاست[EOS][BOM] ره آن پوی که پیدایش ازوست[BOM] لیک با اینهمه خود ناپیداست[EOS][BOM] نتوان گفت که خار از چه دمید[BOM] خار را نیز درین باغ بهاست[EOS][BOM] چرخ با هر که نشاندت بنشین[BOM] هر چه را خواجه روا دید رواست[EOS][BOM] بنده شایسته تنهایی نیست[BOM] حق تعالی\n",
            "y[2]: Tokens: [53, 2404, 998, 55, 9, 7, 113, 92, 359, 4315, 1589, 10984, 7, 197, 43, 11903, 64, 605, 103, 10184, 55, 9, 7, 21651, 56, 105, 1697, 587, 4624, 7, 161, 105, 90, 142, 1424, 9509, 55, 9, 7, 2445, 3777, 53, 116, 3418, 157, 7, 2270, 2398, 53, 116, 1237, 45, 64, 55, 9, 7, 50, 467, 2465, 80, 16159, 7, 51, 161], Decoded: را بسی پسر است[EOS][BOM] اگرت دیده ایست راهی پوی[BOM] چند خندی بر آنکه بی بصر است[EOS][BOM] نیکنامی ز نیک کاری زاد[BOM] نه ز هر نام شخص نامور است[EOS][BOM] خویشتن خواه را چه معرفتست[BOM] شاخه عجب را چه برگ و بر است[EOS][BOM] از سخن گفتن تو دانستم[BOM] که نه\n",
            "y[3]: Tokens: [3964, 7, 999, 3474, 1975, 78, 54, 423, 1365, 681, 1433, 9, 7, 6098, 50, 7551, 78, 7234, 123, 135, 5908, 50, 4946, 7, 128, 204, 46, 2585, 71, 2871, 209, 681, 1433, 9, 7, 197, 244, 15857, 3396, 64, 2759, 2298, 83, 11771, 7, 5093, 4670, 208, 221, 9773, 56, 999, 19022, 681, 1433, 9, 7, 5291, 54, 1086, 3492, 105, 3163, 129], Decoded: گهی[BOM] کز سیه بخت من این خواب گران بیرون رود[EOS][BOM] بگذر از بالین من کاسان شود مردن از آنک[BOM] دل چو در حسرت بود دشوار جان بیرون رود[EOS][BOM] چند بپسندی ستم برجان خسرو هم بترس[BOM] زانکه نیاد بازتیری کزکمان بیرون رود[EOS][BOM] یارب این اندیشه جانان ز جانم چون\n",
            "y[4]: Tokens: [167, 43, 60, 7, 1173, 60, 43, 11422, 73, 57, 1594, 43, 60, 9, 7, 20803, 1604, 1534, 45, 3032, 105, 114, 7, 103, 6226, 110, 53, 1033, 242, 55, 9, 7, 333, 11164, 73, 2335, 73, 2754, 43, 60, 7, 11918, 57, 5656, 56, 11410, 43, 60, 9, 7, 54, 467, 60, 1090, 80, 1033, 254, 7, 80, 116, 3593, 19601, 56, 647], Decoded: راه ها[BOM] اشکها آمیختم با آه ها[EOS][BOM] بدره زر دیدم و رفتم ز دست[BOM] بی تامل روز را گفتم شب است[EOS][BOM] حق نهفتم بافتم افسانه ها[BOM] سوختم با تهمتی کاشانه ها[EOS][BOM] این سخنها بهر تو گفتم تمام[BOM] تو چه گفتی آرمیدی صبح\n",
            "\n",
            "Last 5 sequences in y:\n",
            "y[251]: Tokens: [647, 242, 464, 307, 50, 183, 364, 9, 7, 1960, 6537, 73, 213, 1359, 51, 103, 312, 73, 7, 2573, 2036, 46, 126, 78, 51, 239, 3474, 73, 9, 7, 48, 209, 7256, 1779, 128, 6376, 105, 1090, 337, 7, 51, 80, 105, 4502, 78, 45, 78, 105, 4502, 128, 6167, 9, 7, 48, 1535, 6163, 113, 52, 3301, 6376, 3822, 7, 4694, 64], Decoded: صبح شب غم کم از هزار مهم[EOS][BOM] مکن نصیحتم ای آشنا که بی خبرم[BOM] مدار آینه در پیش من که رو سیهم[EOS][BOM] به جان رسیدم ازین دل بکش ز بهر خدا[BOM] که تو ز ننگ من و من ز ننگ دل برهم[EOS][BOM] به جرم عشقم اگر می کشی بکش لیکن[BOM] نویس بر\n",
            "y[252]: Tokens: [9, 7, 285, 1019, 157, 103, 80, 253, 90, 197, 7, 46, 45, 57, 73, 50, 8680, 252, 3042, 157, 9, 7, 98, 4674, 53, 105, 2866, 53, 92, 820, 416, 7, 9785, 335, 51, 7044, 706, 157, 9, 7, 48, 2866, 2264, 239, 74, 2298, 1845, 7, 1219, 22211, 16400, 157, 9, 7, 285, 50, 127, 5621, 4838, 126, 55, 7, 1836, 5623], Decoded: [EOS][BOM] مرا زندانست بی تو خانه هر چند[BOM] در و بام از خیالت پر نگارست[EOS][BOM] دو چشمم را ز کویت راتبه خاک[BOM] زیادت کن که مزد انتظارست[EOS][BOM] به کویت زرد رو شد خسرو آی[BOM] هوای نیکوان ناسازگارست[EOS][BOM] مرا از روی خوبان قبله پیش است[BOM] مسلمانان ندانم\n",
            "y[253]: Tokens: [96, 2653, 7, 64, 7719, 980, 54, 3396, 691, 1984, 9, 7, 209, 48, 500, 76, 1133, 737, 7, 535, 3600, 1517, 83, 691, 1984, 9, 7, 19864, 57, 242, 4102, 9360, 7, 11227, 4524, 80, 46, 601, 691, 1984, 9, 7, 235, 105, 13178, 80, 103, 181, 55, 7, 1191, 105, 5245, 80, 14340, 55, 9, 7, 6338, 43, 58, 10948, 22274, 92], Decoded: یا رب[BOM] برکسی هرگز این ستم رفته ست[EOS][BOM] جان به دنبال او روان کردم[BOM] گر نیاید حیات هم رفته ست[EOS][BOM] خسروا با شب فراق بساز[BOM] کافتاب تو در عدم رفته ست[EOS][BOM] گل ز رخساره تو بی آب است[BOM] مه ز نظاره تو بیتاب است[EOS][BOM] مژه های کژ دلاویزت\n",
            "y[254]: Tokens: [705, 1993, 181, 9, 7, 45, 123, 1253, 2413, 14448, 7, 10508, 6148, 45, 1260, 46, 423, 9, 7, 213, 20753, 53, 73, 1391, 46, 1662, 7, 7541, 51, 1662, 53, 11611, 256, 9, 7, 252, 52, 3647, 45, 46, 3780, 13530, 185, 7, 999, 252, 5848, 80, 252, 15794, 9, 7, 769, 129, 80, 4779, 6791, 7, 3179, 256, 573, 6892, 45, 3179], Decoded: سنگ چشمه آب[EOS][BOM] وان مست شراب ارغوانی[BOM] مخمور فتاد و ماند در خواب[EOS][BOM] ای مرغک رام گشته در دام[BOM] برخیز که دام را گسستند[EOS][BOM] پر میزن و در سپهر بخرام[BOM] کز پر شکن تو پر شکستند[EOS][BOM] بس چون تو پرندگان گمنام[BOM] جستند ره خلاص و جست\n",
            "y[255]: Tokens: [1144, 80, 352, 46, 5978, 52, 4161, 7, 105, 4339, 405, 91, 13217, 52, 4161, 9, 7, 1063, 105, 1479, 80, 2296, 1594, 5684, 56, 7, 51, 54, 1476, 4274, 91, 46, 5978, 52, 4161, 9, 7, 48, 2501, 2615, 128, 78, 918, 4686, 1222, 246, 7, 204, 2548, 2548, 3560, 999, 2079, 52, 4161, 9, 7, 107, 8744, 51, 128, 3249, 1984, 1696], Decoded: بوی تواش در دماغ می افتد[BOM] ز زندگانی خویشش فراغ می افتد[EOS][BOM] شدم ز زلف تو دیوانه آه مسکینی[BOM] که این خیال کجش در دماغ می افتد[EOS][BOM] به قطره سوز دل من همی کشد زین چشم[BOM] چو شعله شعله گلی کز چراغ می افتد[EOS][BOM] نمی زید که دل سوخته ست خوردن\n",
            "\n",
            "Validation Data:\n",
            "First and Last 5 data points in x:\n",
            "\n",
            "First 5 sequences in x:\n",
            "x[0]: Tokens: [405, 312, 121, 285, 7, 14917, 335, 51, 20001, 167, 2412, 121, 285, 9, 7, 535, 3073, 46, 133, 1407, 15444, 1433, 121, 2398, 7, 133, 3596, 65, 80, 532, 464, 133, 8199, 285, 9, 7, 103, 3364, 1173, 918, 212, 73, 45, 235, 52, 6143, 7, 321, 1779, 106, 1641, 106, 977, 121, 285, 9, 7, 5392, 132, 1222, 2708, 134, 51, 811], Decoded: خویش خبر نیست مرا[BOM] گذری کن که زغم راه گذر نیست مرا[EOS][BOM] گر سرم در سر سو دات رود نیست عجب[BOM] سرسوای تو دارم غم سرنیست مرا[EOS][BOM] بی رخت اشک همی بارم و گل می کارم[BOM] غیر ازین کار کنون کار دگر نیست مرا[EOS][BOM] نازنینا زین هوس مردم که خلق\n",
            "x[1]: Tokens: [9, 7, 50, 78, 149, 92, 48, 246, 5988, 7, 15879, 8175, 80, 183, 12165, 9, 7, 535, 89, 105, 828, 3872, 207, 430, 7, 57, 127, 80, 10192, 72, 4699, 9, 7, 465, 204, 12761, 80, 5106, 7, 5151, 116, 6361, 56, 50, 600, 9, 7, 3593, 758, 1364, 2220, 53, 9702, 7, 61, 1524, 45, 1609, 1524, 80, 1050, 9, 7, 535], Decoded: [EOS][BOM] از من نظرت به چشم سوزن[BOM] واندر دف تو هزار روزن[EOS][BOM] گر ما ز هنر تهی میانیم[BOM] با روی تو بگوی تا بدانیم[EOS][BOM] نبود چو فسانه تو نامی[BOM] بیهوده چه لافی از نظامی[EOS][BOM] گفتی دم اوست مرده رازیست[BOM] آن زان ویست زان تو چیست[EOS][BOM] گر\n",
            "x[2]: Tokens: [459, 51, 2296, 43, 148, 50, 78, 7129, 9, 7, 530, 45, 1274, 51, 46, 979, 1612, 45, 1172, 256, 7, 161, 5817, 256, 51, 12399, 1473, 9655, 2982, 256, 9, 7, 20278, 213, 51, 573, 209, 107, 1615, 4611, 7, 116, 13426, 256, 744, 51, 128, 918, 43, 12034, 9, 7, 48, 4646, 2643, 1617, 6646, 7924, 6181, 7, 51, 90, 6614, 51], Decoded: گیرد که دیوانه تر از من گردند[EOS][BOM] جوان و پیر که در بند مال و فرزندند[BOM] نه عاقلند که طفلان ناخردمندند[EOS][BOM] بخانه ای که ره جان نمی توان بستن[BOM] چه ابلهند کسانی که دل همی بندند[EOS][BOM] به سبزه زار فلک طرفه باغبانانند[BOM] که هر نهال که\n",
            "x[3]: Tokens: [574, 1364, 9, 7, 741, 48, 280, 434, 46, 2708, 127, 415, 7, 1237, 9676, 4058, 131, 103, 646, 3493, 56, 415, 9, 7, 535, 120, 664, 683, 4356, 89, 1261, 2388, 7, 314, 6311, 74, 89, 105, 133, 10428, 415, 9, 7, 4838, 162, 123, 3296, 71, 46, 176, 7, 4838, 6045, 121, 440, 2020, 5875, 415, 9, 7, 213, 657, 647, 758], Decoded: بلکه اوست[EOS][BOM] عمر به پایان رسید در هوس روی دوست[BOM] برگ صبوری کراست بی رخ نیکوی دوست[EOS][BOM] گر همه عالم شوند منکر ما گو شوید[BOM] دور نخواهیم شد ما ز سرکوی دوست[EOS][BOM] قبله اسلامیان کعبه بود در جهان[BOM] قبله عشاق نیست جز خم ابروی دوست[EOS][BOM] ای نفس صبح دم\n",
            "x[4]: Tokens: [4792, 92, 6859, 6143, 48, 10417, 7, 10505, 129, 161, 12764, 161, 13936, 73, 9, 7, 6892, 78, 13246, 697, 213, 16765, 7, 51, 78, 46, 3841, 525, 76, 18165, 9, 7, 3593, 98, 246, 45, 98, 10233, 1222, 60, 1107, 836, 559, 92, 7, 10344, 113, 116, 372, 78, 90, 4117, 52, 836, 10581, 9, 7, 1908, 3229, 999, 1144, 76, 103, 67], Decoded: گلزارت گنه کارم به بویی[BOM] مکش چون نه بدیدم نه چشیدم[EOS][BOM] خلاص من بجویید ای رفیقان[BOM] که من در قید مهر او اسیرم[EOS][BOM] گفتی دو چشم و دو لبم زینها کدام آید خوشت[BOM] خوردند اگر چه خون من هر چار می آید خوشم[EOS][BOM] خواهم شبی کز بوی او بی خود\n",
            "\n",
            "Last 5 sequences in x:\n",
            "x[251]: Tokens: [3663, 7, 4358, 48, 2926, 1153, 9, 7, 204, 2489, 23826, 2438, 3853, 7, 1014, 15832, 56, 75, 57, 10681, 2475, 9, 7, 105, 133, 60, 1716, 432, 45, 8098, 432, 7, 858, 53, 3648, 128, 50, 114, 432, 9, 7, 48, 5621, 112, 6592, 777, 7, 51, 90, 63, 48, 8272, 10227, 9387, 4286, 9, 7, 5574, 891, 16683, 15082, 56, 7, 4358], Decoded: بوستان[BOM] ولیکن به همراهی دوستان[EOS][BOM] چو سازنده ارغنون توی نوش[BOM] بدین رهزنی کرد با تاراج هوش[EOS][BOM] ز سرها خرد رفت و سرمست رفت[BOM] ملک را عنان دل از دست رفت[EOS][BOM] به خوبان دیگر اشارت نمود[BOM] که هر یک به سویی چمیدند زو[EOS][BOM] نهی گشت خرگاه شاهنشهی[BOM] ولیکن\n",
            "x[252]: Tokens: [7, 213, 4322, 2766, 46, 2412, 50, 4322, 103, 10980, 7, 3712, 51, 1253, 53, 465, 8053, 56, 9, 7, 6117, 2952, 117, 1157, 53, 465, 5777, 7, 19812, 16444, 56, 117, 9793, 56, 43, 81, 20329, 3544, 9, 7, 5777, 51, 71, 50, 238, 3163, 48, 5096, 61, 7, 67, 209, 465, 1279, 103, 3783, 792, 5777, 9, 7, 8082, 2203, 45, 11824], Decoded: [BOM] ای پند گوی در گذر از پند بی دلان[BOM] دانی که مست را نبود استقامتی[EOS][BOM] بیکار دلی باشد کو را نبود دردی[BOM] کاهل فرسی باشد کزوی نجهد گردی[EOS][BOM] دردی که بود از عشق جانم به فدای آن[BOM] خود جان نبود شیرین بی ذوق چنان دردی[EOS][BOM] شبها منم و شمعی\n",
            "x[253]: Tokens: [769, 7, 133, 16025, 224, 18211, 75, 9, 7, 48, 884, 311, 45, 133, 224, 3754, 75, 7, 204, 46, 3754, 74, 75, 4310, 1702, 9, 7, 48, 3754, 6880, 432, 129, 7402, 7, 1542, 75, 46, 4745, 61, 1575, 6085, 9, 7, 4268, 45, 3532, 668, 134, 4725, 7, 900, 18892, 56, 46, 997, 46, 1031, 2297, 9, 7, 8695, 3344, 204, 8572], Decoded: بس[BOM] سر کوهکن سوی کهسار کرد[EOS][BOM] به کوه آمد و سر سوی غار کرد[BOM] چو در غار شد کرد مرکب رها[EOS][BOM] به غار اندرون رفت چون اژدها[BOM] نگه کرد در کنج آن تنگ نای[EOS][BOM] فرشته وشی دید مردم نمای[BOM] لگیمی در آورده در گرد دوش[EOS][BOM] خزیده چو روباه\n",
            "x[254]: Tokens: [15821, 45, 2123, 432, 9, 7, 45, 123, 792, 582, 51, 46, 5247, 45, 11613, 7, 1534, 18127, 56, 8487, 48, 416, 9, 7, 6554, 7986, 71, 63, 1407, 1561, 7, 224, 112, 3388, 2372, 3141, 9, 7, 90, 116, 46, 101, 12617, 3141, 48, 7670, 7, 920, 737, 51, 2123, 55, 161, 6156, 9, 7, 45, 123, 2082, 224, 91, 6554, 777, 4107], Decoded: انگبین و روغن رفت[EOS][BOM] وان چنان بد که در خس و خاشاک[BOM] دیدم آلایشی چکیده به خاک[EOS][BOM] مگس افکنده بود یک سو شور[BOM] سوی دیگر قطار لشکر مور[EOS][BOM] هر چه در وی دوید مور به جهد[BOM] حکم کردم که روغن است نه شهد[EOS][BOM] وانچه سویش مگس نمود هجوم\n",
            "x[255]: Tokens: [432, 7, 535, 116, 606, 305, 1085, 2142, 566, 9, 7, 103, 635, 467, 11718, 14250, 7, 103, 2556, 1117, 902, 53, 627, 14250, 9, 7, 1908, 67, 53, 831, 7095, 3822, 7, 3239, 4274, 157, 45, 359, 3448, 14250, 9, 7, 61, 116, 64, 78, 724, 80, 2233, 213, 305, 78, 95, 7, 52, 15115, 51, 566, 64, 6333, 100, 6461, 9, 7], Decoded: رفت[BOM] گر چه صد جای سینه پاره کنم[EOS][BOM] بی معرفی سخن مسلسل چکنم[BOM] بی قوت عقل نکته را حل چکنم[EOS][BOM] خواهم خود را درست بینم لیکن[BOM] آیینه کجست و دیده احوال چکنم[EOS][BOM] آن چه بر من لب تو میکند ای جای من نیز[BOM] می توانم که کنم بر لبت اما نکنم[EOS][BOM]\n",
            "\n",
            "First and Last 5 data points in y:\n",
            "\n",
            "First 5 sequences in y:\n",
            "y[0]: Tokens: [312, 121, 285, 7, 14917, 335, 51, 20001, 167, 2412, 121, 285, 9, 7, 535, 3073, 46, 133, 1407, 15444, 1433, 121, 2398, 7, 133, 3596, 65, 80, 532, 464, 133, 8199, 285, 9, 7, 103, 3364, 1173, 918, 212, 73, 45, 235, 52, 6143, 7, 321, 1779, 106, 1641, 106, 977, 121, 285, 9, 7, 5392, 132, 1222, 2708, 134, 51, 811, 7], Decoded: خبر نیست مرا[BOM] گذری کن که زغم راه گذر نیست مرا[EOS][BOM] گر سرم در سر سو دات رود نیست عجب[BOM] سرسوای تو دارم غم سرنیست مرا[EOS][BOM] بی رخت اشک همی بارم و گل می کارم[BOM] غیر ازین کار کنون کار دگر نیست مرا[EOS][BOM] نازنینا زین هوس مردم که خلق[BOM]\n",
            "y[1]: Tokens: [7, 50, 78, 149, 92, 48, 246, 5988, 7, 15879, 8175, 80, 183, 12165, 9, 7, 535, 89, 105, 828, 3872, 207, 430, 7, 57, 127, 80, 10192, 72, 4699, 9, 7, 465, 204, 12761, 80, 5106, 7, 5151, 116, 6361, 56, 50, 600, 9, 7, 3593, 758, 1364, 2220, 53, 9702, 7, 61, 1524, 45, 1609, 1524, 80, 1050, 9, 7, 535, 1524], Decoded: [BOM] از من نظرت به چشم سوزن[BOM] واندر دف تو هزار روزن[EOS][BOM] گر ما ز هنر تهی میانیم[BOM] با روی تو بگوی تا بدانیم[EOS][BOM] نبود چو فسانه تو نامی[BOM] بیهوده چه لافی از نظامی[EOS][BOM] گفتی دم اوست مرده رازیست[BOM] آن زان ویست زان تو چیست[EOS][BOM] گر زان\n",
            "y[2]: Tokens: [51, 2296, 43, 148, 50, 78, 7129, 9, 7, 530, 45, 1274, 51, 46, 979, 1612, 45, 1172, 256, 7, 161, 5817, 256, 51, 12399, 1473, 9655, 2982, 256, 9, 7, 20278, 213, 51, 573, 209, 107, 1615, 4611, 7, 116, 13426, 256, 744, 51, 128, 918, 43, 12034, 9, 7, 48, 4646, 2643, 1617, 6646, 7924, 6181, 7, 51, 90, 6614, 51, 1630], Decoded: که دیوانه تر از من گردند[EOS][BOM] جوان و پیر که در بند مال و فرزندند[BOM] نه عاقلند که طفلان ناخردمندند[EOS][BOM] بخانه ای که ره جان نمی توان بستن[BOM] چه ابلهند کسانی که دل همی بندند[EOS][BOM] به سبزه زار فلک طرفه باغبانانند[BOM] که هر نهال که شان\n",
            "y[3]: Tokens: [1364, 9, 7, 741, 48, 280, 434, 46, 2708, 127, 415, 7, 1237, 9676, 4058, 131, 103, 646, 3493, 56, 415, 9, 7, 535, 120, 664, 683, 4356, 89, 1261, 2388, 7, 314, 6311, 74, 89, 105, 133, 10428, 415, 9, 7, 4838, 162, 123, 3296, 71, 46, 176, 7, 4838, 6045, 121, 440, 2020, 5875, 415, 9, 7, 213, 657, 647, 758, 535], Decoded: اوست[EOS][BOM] عمر به پایان رسید در هوس روی دوست[BOM] برگ صبوری کراست بی رخ نیکوی دوست[EOS][BOM] گر همه عالم شوند منکر ما گو شوید[BOM] دور نخواهیم شد ما ز سرکوی دوست[EOS][BOM] قبله اسلامیان کعبه بود در جهان[BOM] قبله عشاق نیست جز خم ابروی دوست[EOS][BOM] ای نفس صبح دم گر\n",
            "y[4]: Tokens: [92, 6859, 6143, 48, 10417, 7, 10505, 129, 161, 12764, 161, 13936, 73, 9, 7, 6892, 78, 13246, 697, 213, 16765, 7, 51, 78, 46, 3841, 525, 76, 18165, 9, 7, 3593, 98, 246, 45, 98, 10233, 1222, 60, 1107, 836, 559, 92, 7, 10344, 113, 116, 372, 78, 90, 4117, 52, 836, 10581, 9, 7, 1908, 3229, 999, 1144, 76, 103, 67, 1967], Decoded: ت گنه کارم به بویی[BOM] مکش چون نه بدیدم نه چشیدم[EOS][BOM] خلاص من بجویید ای رفیقان[BOM] که من در قید مهر او اسیرم[EOS][BOM] گفتی دو چشم و دو لبم زینها کدام آید خوشت[BOM] خوردند اگر چه خون من هر چار می آید خوشم[EOS][BOM] خواهم شبی کز بوی او بی خود شوم\n",
            "\n",
            "Last 5 sequences in y:\n",
            "y[251]: Tokens: [7, 4358, 48, 2926, 1153, 9, 7, 204, 2489, 23826, 2438, 3853, 7, 1014, 15832, 56, 75, 57, 10681, 2475, 9, 7, 105, 133, 60, 1716, 432, 45, 8098, 432, 7, 858, 53, 3648, 128, 50, 114, 432, 9, 7, 48, 5621, 112, 6592, 777, 7, 51, 90, 63, 48, 8272, 10227, 9387, 4286, 9, 7, 5574, 891, 16683, 15082, 56, 7, 4358, 2126], Decoded: [BOM] ولیکن به همراهی دوستان[EOS][BOM] چو سازنده ارغنون توی نوش[BOM] بدین رهزنی کرد با تاراج هوش[EOS][BOM] ز سرها خرد رفت و سرمست رفت[BOM] ملک را عنان دل از دست رفت[EOS][BOM] به خوبان دیگر اشارت نمود[BOM] که هر یک به سویی چمیدند زو[EOS][BOM] نهی گشت خرگاه شاهنشهی[BOM] ولیکن شه\n",
            "y[252]: Tokens: [213, 4322, 2766, 46, 2412, 50, 4322, 103, 10980, 7, 3712, 51, 1253, 53, 465, 8053, 56, 9, 7, 6117, 2952, 117, 1157, 53, 465, 5777, 7, 19812, 16444, 56, 117, 9793, 56, 43, 81, 20329, 3544, 9, 7, 5777, 51, 71, 50, 238, 3163, 48, 5096, 61, 7, 67, 209, 465, 1279, 103, 3783, 792, 5777, 9, 7, 8082, 2203, 45, 11824, 83], Decoded: ای پند گوی در گذر از پند بی دلان[BOM] دانی که مست را نبود استقامتی[EOS][BOM] بیکار دلی باشد کو را نبود دردی[BOM] کاهل فرسی باشد کزوی نجهد گردی[EOS][BOM] دردی که بود از عشق جانم به فدای آن[BOM] خود جان نبود شیرین بی ذوق چنان دردی[EOS][BOM] شبها منم و شمعی هم\n",
            "y[253]: Tokens: [7, 133, 16025, 224, 18211, 75, 9, 7, 48, 884, 311, 45, 133, 224, 3754, 75, 7, 204, 46, 3754, 74, 75, 4310, 1702, 9, 7, 48, 3754, 6880, 432, 129, 7402, 7, 1542, 75, 46, 4745, 61, 1575, 6085, 9, 7, 4268, 45, 3532, 668, 134, 4725, 7, 900, 18892, 56, 46, 997, 46, 1031, 2297, 9, 7, 8695, 3344, 204, 8572, 24598], Decoded: [BOM] سر کوهکن سوی کهسار کرد[EOS][BOM] به کوه آمد و سر سوی غار کرد[BOM] چو در غار شد کرد مرکب رها[EOS][BOM] به غار اندرون رفت چون اژدها[BOM] نگه کرد در کنج آن تنگ نای[EOS][BOM] فرشته وشی دید مردم نمای[BOM] لگیمی در آورده در گرد دوش[EOS][BOM] خزیده چو روباه پشمینه\n",
            "y[254]: Tokens: [45, 2123, 432, 9, 7, 45, 123, 792, 582, 51, 46, 5247, 45, 11613, 7, 1534, 18127, 56, 8487, 48, 416, 9, 7, 6554, 7986, 71, 63, 1407, 1561, 7, 224, 112, 3388, 2372, 3141, 9, 7, 90, 116, 46, 101, 12617, 3141, 48, 7670, 7, 920, 737, 51, 2123, 55, 161, 6156, 9, 7, 45, 123, 2082, 224, 91, 6554, 777, 4107, 7], Decoded: و روغن رفت[EOS][BOM] وان چنان بد که در خس و خاشاک[BOM] دیدم آلایشی چکیده به خاک[EOS][BOM] مگس افکنده بود یک سو شور[BOM] سوی دیگر قطار لشکر مور[EOS][BOM] هر چه در وی دوید مور به جهد[BOM] حکم کردم که روغن است نه شهد[EOS][BOM] وانچه سویش مگس نمود هجوم[BOM]\n",
            "y[255]: Tokens: [7, 535, 116, 606, 305, 1085, 2142, 566, 9, 7, 103, 635, 467, 11718, 14250, 7, 103, 2556, 1117, 902, 53, 627, 14250, 9, 7, 1908, 67, 53, 831, 7095, 3822, 7, 3239, 4274, 157, 45, 359, 3448, 14250, 9, 7, 61, 116, 64, 78, 724, 80, 2233, 213, 305, 78, 95, 7, 52, 15115, 51, 566, 64, 6333, 100, 6461, 9, 7, 2297], Decoded: [BOM] گر چه صد جای سینه پاره کنم[EOS][BOM] بی معرفی سخن مسلسل چکنم[BOM] بی قوت عقل نکته را حل چکنم[EOS][BOM] خواهم خود را درست بینم لیکن[BOM] آیینه کجست و دیده احوال چکنم[EOS][BOM] آن چه بر من لب تو میکند ای جای من نیز[BOM] می توانم که کنم بر لبت اما نکنم[EOS][BOM] دوش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572ed86e-ac81-4a4e-8f1d-1a18202d05a6",
      "metadata": {
        "id": "572ed86e-ac81-4a4e-8f1d-1a18202d05a6"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "        # Ensure the data is a torch tensor\n",
        "        if not isinstance(self.data, torch.Tensor):\n",
        "            self.data = torch.tensor(self.data, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + 1 + self.block_size]\n",
        "        return x, y\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # Weight tying\n",
        "\n",
        "        # Init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # Report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
        "\n",
        "        # Forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484985e9-f86f-4f5f-8d45-717d6b54c1a2",
      "metadata": {
        "id": "484985e9-f86f-4f5f-8d45-717d6b54c1a2",
        "outputId": "f3b7ac05-7302-40cb-b6da-d6d4ab6f56ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 3.99M\n",
            "GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(25000, 128)\n",
            "    (wpe): Embedding(64, 128)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-3): 4 x Block(\n",
            "        (ln_1): LayerNorm()\n",
            "        (attn): CausalSelfAttention(\n",
            "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
            "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm()\n",
            "        (mlp): MLP(\n",
            "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=128, out_features=25000, bias=False)\n",
            ")\n",
            "vocab_size= 25000\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int  # n_embd must be divisible by n_head\n",
        "    dropout: float\n",
        "    bias: bool\n",
        "\"\"\"\n",
        "config = GPTConfig(\n",
        "    block_size=64,\n",
        "    vocab_size= vocab_size,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embd=64,  # Ensure n_embd is divisible by n_head\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size=block_size,\n",
        "    vocab_size= vocab_size,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embd=128,  # Ensure n_embd is divisible by n_head\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "\n",
        "model = GPT(config)\n",
        "print(model)\n",
        "\n",
        "print (\"vocab_size=\" , vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84823ac5-2153-4215-a016-1c6d6200e9b0",
      "metadata": {
        "id": "84823ac5-2153-4215-a016-1c6d6200e9b0",
        "outputId": "81cfe17c-6280-4b7f-99ad-98c89fce9671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 18, with 3,994,624 parameters\n",
            "num non-decayed parameter tensors: 34, with 6,912 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize optimizer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)  # Ensure the model is on the correct device\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95), device_type=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4f1dba-5738-4bad-9cc4-ca6ff0cb62f9",
      "metadata": {
        "id": "9d4f1dba-5738-4bad-9cc4-ca6ff0cb62f9"
      },
      "outputs": [],
      "source": [
        "# Define the learning rate scheduler\n",
        "def get_lr(it):\n",
        "    # 1) Linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) If it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) In between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# Function to handle user input in a separate thread\n",
        "def check_for_stop(stop_queue):\n",
        "    while True:\n",
        "        user_input = input()\n",
        "        if user_input.lower() == 'q':\n",
        "            stop_queue.put(True)\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a queue to communicate between the input thread and the main thread\n",
        "stop_queue = queue.Queue()\n",
        "\n",
        "# Start the input thread\n",
        "input_thread = threading.Thread(target=check_for_stop, args=(stop_queue,))\n",
        "input_thread.daemon = True\n",
        "input_thread.start()\n",
        "\n",
        "# Define global configurations and hyperparameters\n",
        "\n",
        "config = {\n",
        "    'learning_rate': 3e-4,\n",
        "    'always_save_checkpoint': True,\n",
        "    'out_dir': './checkpoints',\n",
        "    'eval_only': False,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'eval_interval': 200,\n",
        "    'max_iters': 20000,\n",
        "    'log_interval': 10,\n",
        "    'grad_clip': 0.1,\n",
        "    'decay_lr': False,\n",
        "    'max_epochs': 50,\n",
        "    'target_val_loss': 2  # Example target validation loss\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Disable scaler and autocast if CUDA is not available\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "iter_num = 0\n",
        "best_val_loss = float('inf')\n",
        "epoch = 0\n",
        "\n",
        "while epoch < config['max_epochs']:\n",
        "    train_loader = get_data_loader(train_ids, batch_size, block_size)\n",
        "    val_loader = get_data_loader(val_ids, batch_size, block_size)\n",
        "\n",
        "    for batch_idx, (X, Y) in enumerate(train_loader):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        # Determine and set the learning rate for this iteration\n",
        "        lr = get_lr(iter_num) if config['decay_lr'] else config['learning_rate']\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Forward, backward update with gradient accumulation\n",
        "        for micro_step in range(config['gradient_accumulation_steps']):\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / config['gradient_accumulation_steps']  # scale the loss to account for gradient accumulation\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "        # Clip the gradient\n",
        "        if config['grad_clip'] != 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
        "        # Step the optimizer and scaler\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        iter_num += 1\n",
        "\n",
        "        # Print loss every 50 iterations\n",
        "        if iter_num % 50 == 0:\n",
        "            print(f\"Iteration {iter_num}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "        # Evaluate the loss on train/val sets and write checkpoints\n",
        "        if iter_num % config['eval_interval'] == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for val_X, val_Y in val_loader:\n",
        "                    val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
        "                    logits, loss = model(val_X, val_Y)\n",
        "                    val_loss += loss.item()\n",
        "            val_loss /= len(val_loader)\n",
        "            model.train()\n",
        "\n",
        "            print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n",
        "            if val_loss < best_val_loss or config['always_save_checkpoint']:\n",
        "                best_val_loss = val_loss\n",
        "                if iter_num > 0:\n",
        "                    checkpoint = {\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'config': config,\n",
        "                        'iter_num': iter_num,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                    }\n",
        "                    #print(f\"saving checkpoint to {config['out_dir']}\")\n",
        "                    #torch.save(checkpoint, os.path.join(config['out_dir'], 'ckpt.pt'))\n",
        "            if val_loss < config['target_val_loss']:\n",
        "                print(\"Target validation loss achieved. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        # Termination condition\n",
        "        if iter_num >= config['max_iters']:\n",
        "            break\n",
        "\n",
        "    if not stop_queue.empty():\n",
        "        break\n",
        "\n",
        "    epoch += 1\n",
        "    if iter_num >= config['max_iters']:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jFMlzfTEi8Am",
        "outputId": "7afc1ec7-5975-463c-8454-3363360b131c"
      },
      "id": "jFMlzfTEi8Am",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 50, Loss: 1.9202301502227783\n",
            "Iteration 100, Loss: 1.6358609199523926\n",
            "Iteration 150, Loss: 1.542484998703003\n",
            "Iteration 200, Loss: 1.4966806173324585\n",
            "step 200: val loss 5.9998\n",
            "Iteration 250, Loss: 1.4721651077270508\n",
            "Iteration 300, Loss: 1.4386447668075562\n",
            "Iteration 350, Loss: 1.4046943187713623\n",
            "Iteration 400, Loss: 1.3978220224380493\n",
            "step 400: val loss 5.6523\n",
            "Iteration 450, Loss: 1.3835545778274536\n",
            "Iteration 500, Loss: 1.3691529035568237\n",
            "Iteration 550, Loss: 1.3690887689590454\n",
            "Iteration 600, Loss: 1.359129786491394\n",
            "step 600: val loss 5.5089\n",
            "Iteration 650, Loss: 1.3417115211486816\n",
            "Iteration 700, Loss: 1.334824800491333\n",
            "Iteration 750, Loss: 1.3214160203933716\n",
            "Iteration 800, Loss: 1.3144093751907349\n",
            "step 800: val loss 5.4175\n",
            "Iteration 850, Loss: 1.304896593093872\n",
            "Iteration 900, Loss: 1.3021924495697021\n",
            "Iteration 950, Loss: 1.2948496341705322\n",
            "Iteration 1000, Loss: 1.293408751487732\n",
            "step 1000: val loss 5.3308\n",
            "Iteration 1050, Loss: 1.2786697149276733\n",
            "Iteration 1100, Loss: 1.2670812606811523\n",
            "Iteration 1150, Loss: 1.254096269607544\n",
            "Iteration 1200, Loss: 1.2585506439208984\n",
            "step 1200: val loss 5.2474\n",
            "Iteration 1250, Loss: 1.243360996246338\n",
            "Iteration 1300, Loss: 1.24403715133667\n",
            "Iteration 1350, Loss: 1.233591079711914\n",
            "Iteration 1400, Loss: 1.2299296855926514\n",
            "step 1400: val loss 5.1935\n",
            "Iteration 1450, Loss: 1.2163424491882324\n",
            "Iteration 1500, Loss: 1.2039185762405396\n",
            "Iteration 1550, Loss: 1.2046936750411987\n",
            "Iteration 1600, Loss: 1.200524926185608\n",
            "step 1600: val loss 5.1445\n",
            "Iteration 1650, Loss: 1.1961671113967896\n",
            "Iteration 1700, Loss: 1.1915169954299927\n",
            "Iteration 1750, Loss: 1.192723274230957\n",
            "Iteration 1800, Loss: 1.1833032369613647\n",
            "step 1800: val loss 5.1020\n",
            "Iteration 1850, Loss: 1.1844305992126465\n",
            "Iteration 1900, Loss: 1.165651559829712\n",
            "Iteration 1950, Loss: 1.153791069984436\n",
            "Iteration 2000, Loss: 1.1583173274993896\n",
            "step 2000: val loss 5.0671\n",
            "Iteration 2050, Loss: 1.1481224298477173\n",
            "Iteration 2100, Loss: 1.150804042816162\n",
            "Iteration 2150, Loss: 1.1433184146881104\n",
            "Iteration 2200, Loss: 1.1280786991119385\n",
            "step 2200: val loss 5.0405\n",
            "Iteration 2250, Loss: 1.1263850927352905\n",
            "Iteration 2300, Loss: 1.1260939836502075\n",
            "Iteration 2350, Loss: 1.1223244667053223\n",
            "Iteration 2400, Loss: 1.1254651546478271\n",
            "step 2400: val loss 5.0214\n",
            "Iteration 2450, Loss: 1.1201359033584595\n",
            "Iteration 2500, Loss: 1.1120485067367554\n",
            "Iteration 2550, Loss: 1.1091116666793823\n",
            "Iteration 2600, Loss: 1.105168104171753\n",
            "step 2600: val loss 5.0001\n",
            "Iteration 2650, Loss: 1.0982404947280884\n",
            "Iteration 2700, Loss: 1.0986018180847168\n",
            "Iteration 2750, Loss: 1.080701470375061\n",
            "Iteration 2800, Loss: 1.0941959619522095\n",
            "step 2800: val loss 4.9820\n",
            "Iteration 2850, Loss: 1.0917199850082397\n",
            "Iteration 2900, Loss: 1.0880248546600342\n",
            "Iteration 2950, Loss: 1.0771232843399048\n",
            "Iteration 3000, Loss: 1.075701355934143\n",
            "step 3000: val loss 4.9785\n",
            "Iteration 3050, Loss: 1.0712140798568726\n",
            "Iteration 3100, Loss: 1.06571364402771\n",
            "Iteration 3150, Loss: 1.062978744506836\n",
            "Iteration 3200, Loss: 1.060146689414978\n",
            "step 3200: val loss 4.9647\n",
            "Iteration 3250, Loss: 1.0599744319915771\n",
            "Iteration 3300, Loss: 1.046773076057434\n",
            "Iteration 3350, Loss: 1.046205759048462\n",
            "Iteration 3400, Loss: 1.049574851989746\n",
            "step 3400: val loss 4.9620\n",
            "Iteration 3450, Loss: 1.038312554359436\n",
            "Iteration 3500, Loss: 1.0288398265838623\n",
            "Iteration 3550, Loss: 1.0412384271621704\n",
            "Iteration 3600, Loss: 1.0223870277404785\n",
            "step 3600: val loss 4.9432\n",
            "Iteration 3650, Loss: 1.027020812034607\n",
            "Iteration 3700, Loss: 1.0272291898727417\n",
            "Iteration 3750, Loss: 1.0087310075759888\n",
            "Iteration 3800, Loss: 1.0140687227249146\n",
            "step 3800: val loss 4.9437\n",
            "Iteration 3850, Loss: 1.0152207612991333\n",
            "Iteration 3900, Loss: 1.0106220245361328\n",
            "Iteration 3950, Loss: 1.006667137145996\n",
            "Iteration 4000, Loss: 0.9985241889953613\n",
            "step 4000: val loss 4.9505\n",
            "Iteration 4050, Loss: 0.999215841293335\n",
            "Iteration 4100, Loss: 0.9937205910682678\n",
            "Iteration 4150, Loss: 1.0021257400512695\n",
            "Iteration 4200, Loss: 0.9818969964981079\n",
            "step 4200: val loss 4.9417\n",
            "Iteration 4250, Loss: 0.988480806350708\n",
            "Iteration 4300, Loss: 0.9896795153617859\n",
            "Iteration 4350, Loss: 0.9795513153076172\n",
            "Iteration 4400, Loss: 0.9831405878067017\n",
            "step 4400: val loss 4.9564\n",
            "Iteration 4450, Loss: 0.9866148233413696\n",
            "Iteration 4500, Loss: 0.973725438117981\n",
            "Iteration 4550, Loss: 0.9657113552093506\n",
            "Iteration 4600, Loss: 0.9794155359268188\n",
            "step 4600: val loss 4.9565\n",
            "Iteration 4650, Loss: 0.9644448757171631\n",
            "Iteration 4700, Loss: 0.9731197953224182\n",
            "Iteration 4750, Loss: 0.97044837474823\n",
            "Iteration 4800, Loss: 0.966501772403717\n",
            "step 4800: val loss 4.9437\n",
            "Iteration 4850, Loss: 0.9661837816238403\n",
            "Iteration 4900, Loss: 0.9612259864807129\n",
            "Iteration 4950, Loss: 0.9537442326545715\n",
            "Iteration 5000, Loss: 0.9558874368667603\n",
            "step 5000: val loss 4.9506\n",
            "Iteration 5050, Loss: 0.9506038427352905\n",
            "Iteration 5100, Loss: 0.9505434632301331\n",
            "Iteration 5150, Loss: 0.9435268640518188\n",
            "Iteration 5200, Loss: 0.9532144665718079\n",
            "step 5200: val loss 4.9755\n",
            "Iteration 5250, Loss: 0.9488765001296997\n",
            "Iteration 5300, Loss: 0.9348562955856323\n",
            "Iteration 5350, Loss: 0.9412350058555603\n",
            "Iteration 5400, Loss: 0.9366230964660645\n",
            "step 5400: val loss 4.9769\n",
            "Iteration 5450, Loss: 0.9438430070877075\n",
            "Iteration 5500, Loss: 0.9311622381210327\n",
            "Iteration 5550, Loss: 0.9354107975959778\n",
            "Iteration 5600, Loss: 0.9302873611450195\n",
            "step 5600: val loss 4.9868\n",
            "Iteration 5650, Loss: 0.9311366677284241\n",
            "Iteration 5700, Loss: 0.931649923324585\n",
            "Iteration 5750, Loss: 0.9260974526405334\n",
            "Iteration 5800, Loss: 0.9210860729217529\n",
            "step 5800: val loss 4.9789\n",
            "Iteration 5850, Loss: 0.9199722409248352\n",
            "Iteration 5900, Loss: 0.9143486618995667\n",
            "Iteration 5950, Loss: 0.9173865914344788\n",
            "Iteration 6000, Loss: 0.9075995683670044\n",
            "step 6000: val loss 4.9871\n",
            "Iteration 6050, Loss: 0.929075300693512\n",
            "Iteration 6100, Loss: 0.9071624279022217\n",
            "Iteration 6150, Loss: 0.9044260382652283\n",
            "Iteration 6200, Loss: 0.906688392162323\n",
            "step 6200: val loss 5.0029\n",
            "Iteration 6250, Loss: 0.9113801717758179\n",
            "Iteration 6300, Loss: 0.8998733162879944\n",
            "Iteration 6350, Loss: 0.9096035361289978\n",
            "Iteration 6400, Loss: 0.907012403011322\n",
            "step 6400: val loss 5.0022\n",
            "Iteration 6450, Loss: 0.8977574706077576\n",
            "Iteration 6500, Loss: 0.8950259685516357\n",
            "Iteration 6550, Loss: 0.9005653858184814\n",
            "Iteration 6600, Loss: 0.894811749458313\n",
            "step 6600: val loss 5.0186\n",
            "Iteration 6650, Loss: 0.8923662900924683\n",
            "Iteration 6700, Loss: 0.8929679989814758\n",
            "Iteration 6750, Loss: 0.8945130705833435\n",
            "q\n",
            "Iteration 6800, Loss: 0.8893240094184875\n",
            "step 6800: val loss 5.0405\n",
            "Iteration 6850, Loss: 0.8872221112251282\n",
            "Iteration 6900, Loss: 0.8807616829872131\n",
            "Iteration 6950, Loss: 0.8923197984695435\n",
            "Iteration 7000, Loss: 0.8825562596321106\n",
            "step 7000: val loss 5.0297\n",
            "Iteration 7050, Loss: 0.8875437378883362\n",
            "Iteration 7100, Loss: 0.8856870532035828\n",
            "Iteration 7150, Loss: 0.8829011917114258\n",
            "Iteration 7200, Loss: 0.8714834451675415\n",
            "step 7200: val loss 5.0348\n",
            "Iteration 7250, Loss: 0.8769721388816833\n",
            "Iteration 7300, Loss: 0.8835225701332092\n",
            "Iteration 7350, Loss: 0.8812079429626465\n",
            "Iteration 7400, Loss: 0.8812879323959351\n",
            "step 7400: val loss 5.0367\n",
            "Iteration 7450, Loss: 0.8712126016616821\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6bdf1ea97db7>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Determine and set the learning rate for this iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69850b19-4e6f-4fe3-908b-4dd315062099",
      "metadata": {
        "id": "69850b19-4e6f-4fe3-908b-4dd315062099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ce2a65-ef36-4990-8d20-73691bca732f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model, optimizer state, and configuration saved to GPTper_saves\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model, optimizer state, and configuration details\n",
        "import json\n",
        "\n",
        "# Save the trained model, optimizer state, and configuration details\n",
        "save_dir = 'GPTper_saves'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save the model state\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, 'final_model_perchin.pt'))\n",
        "\n",
        "# Save the optimizer state\n",
        "torch.save(optimizer.state_dict(), os.path.join(save_dir, 'final_optimizer_perchin.pt'))\n",
        "\n",
        "# Save the configuration\n",
        "config_path = os.path.join(save_dir, 'config_perchin.json')\n",
        "with open(config_path, 'w') as config_file:\n",
        "    json.dump(config, config_file, indent=4)\n",
        "\n",
        "print(f'Model, optimizer state, and configuration saved to {save_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662dfe7d-0d0a-4e1a-9aba-4ca88d848519",
      "metadata": {
        "id": "662dfe7d-0d0a-4e1a-9aba-4ca88d848519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed007a9-9d98-4fe9-8a48-8c0b7e25a270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            "تیر نگاه تو در دل من ‌‌‌‌‌ مسکین‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ چه‌‌‌‌‌ دانی‌‌‌‌‌ که‌‌‌‌‌ به‌‌‌‌‌ یک‌‌‌‌‌ نظاره‌‌‌‌‌ تو‌‌‌‌‌ داری‌‌‌‌‌ [EOS]‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ به‌‌‌‌‌ هر‌‌‌‌‌ ذره‌‌‌‌‌ ای‌‌‌‌‌ که‌‌‌‌‌ مرا‌‌‌‌‌ در‌‌‌‌‌ هوس‌‌‌‌‌ روی‌‌‌‌‌ تو‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ به‌‌‌‌‌ روز‌‌‌‌‌ خویش‌‌‌‌‌ می‌‌‌‌‌ کشم‌‌‌‌‌ و‌‌‌‌‌ شب‌‌‌‌‌ در‌‌‌‌‌ همه‌‌‌‌‌ کس‌‌‌‌‌ بودی‌‌‌‌‌ [EOS]‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ چو‌‌‌‌‌ جان‌‌‌‌‌ دهم‌‌‌‌‌ سخن‌‌‌‌‌ از‌‌‌‌‌ دیدن‌‌‌‌‌ تو‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ به‌‌‌‌‌ جان‌‌‌‌‌ دهم‌‌‌‌‌ و‌‌‌‌‌ آن‌‌‌‌‌ قد‌‌‌‌‌ او‌‌‌‌‌ که‌‌‌‌‌ داری‌‌‌‌‌ [EOS]‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ به‌‌‌‌‌ دل‌‌‌‌‌ رسید‌‌‌‌‌ هجر‌‌‌‌‌ تو‌‌‌‌‌ هر‌‌‌‌‌ دم‌‌‌‌‌ ار‌‌‌‌‌ چه‌‌‌‌‌ هست‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ چو‌‌‌‌‌ نیست‌‌‌‌‌ امید‌‌‌‌‌ زیستن‌‌‌‌‌ بی‌‌‌‌‌ جای‌‌‌‌‌ ما‌‌‌‌‌ بودی‌‌‌‌‌ [EOS]‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ دل‌‌‌‌‌ از‌‌‌‌‌ تو‌‌‌‌‌ بسوخت‌‌‌‌‌ م‌‌‌‌‌ سوز‌‌‌‌‌ و‌‌‌‌‌ گداز‌‌‌‌‌ نتوان‌‌‌‌‌ کرد‌‌‌‌‌ [BOM]\n",
            "‌‌‌‌‌ هنوز‌‌‌‌‌ دلی‌‌‌‌‌ ز‌‌‌‌‌ تو‌‌‌‌‌ خام‌‌‌‌‌ باشد‌‌‌‌‌ [EOS]‌‌‌‌‌ [BOM]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to tokenize input text and truncate to block size\n",
        "def tokenize_and_truncate_text(text, block_size):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(tokens) > block_size:\n",
        "        tokens = tokens[-block_size:]  # Truncate to the last block_size tokens\n",
        "    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to decode output tokens\n",
        "def decode_output(predicted_ids):\n",
        "    predicted_tokens = predicted_ids.tolist()  # Convert tensor to list\n",
        "    predicted_text = tokenizer.decode(predicted_tokens)\n",
        "    return predicted_text\n",
        "\n",
        "# Function to get model output for a single input text\n",
        "def get_model_output(model, input_text, config, temperature=1.0, top_k=50):\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Tokenize and truncate input text\n",
        "    input_ids = tokenize_and_truncate_text(input_text, block_size)\n",
        "    input_ids = input_ids.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(input_ids)\n",
        "        logits = logits[:, -1, :] / temperature  # Apply temperature\n",
        "\n",
        "        # Top-k sampling\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
        "        probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "        # Sample from the distribution\n",
        "        chosen_idx = torch.multinomial(probs, num_samples=1)\n",
        "        predicted_token_id = top_k_indices[0, chosen_idx[0]]\n",
        "\n",
        "        # Decode the predicted token\n",
        "        predicted_text = decode_output(predicted_token_id)\n",
        "\n",
        "    return predicted_text\n",
        "\n",
        "# Function to validate the predicted text\n",
        "def is_valid_token(token):\n",
        "    # Implement your logic to check if the token is valid or not\n",
        "    # This can include checking for known invalid sequences, punctuation, etc.\n",
        "    invalid_sequences = ['ا ی']\n",
        "    return token not in invalid_sequences\n",
        "\n",
        "# Sample input text\n",
        "input_text = \"بنشین که با من هر نظر\"\n",
        "input_text = \"حکیمان پیشین چنین گفته اند\"\n",
        "input_text = \"با نصرت و فتح و ظفر و دولت والا\"\n",
        "input_text = \"توانا بود هر که دانا بود\"\n",
        "input_text = \"تو خوشگل ناز منی \"\n",
        "input_text = \"ای دوست\"\n",
        "input_text = \"ای صبا\"\n",
        "input_text = \"دور است کاروان سحر\"\n",
        "input_text = \"از عشق تو من مردم \"\n",
        "input_text = \"تیر نگاه تو در دل من \"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define maximum BOM count\n",
        "max_bom_count = 10\n",
        "\n",
        "# Accumulate the complete generated text\n",
        "complete_text = input_text\n",
        "\n",
        "# Track the last few generated tokens\n",
        "repetition_threshold = 5\n",
        "last_few_tokens = []\n",
        "temperature = 0.7\n",
        "\n",
        "# Generate text in a loop until the maximum BOM count is reached\n",
        "bom_count = 0\n",
        "\n",
        "while bom_count < max_bom_count:\n",
        "    predicted_text = get_model_output(model, input_text, config, temperature=0.7, top_k=50)\n",
        "\n",
        "    # Prevent repeating sequences and filter invalid tokens\n",
        "    if predicted_text.strip() in last_few_tokens or not is_valid_token(predicted_text.strip()):\n",
        "        temperature *= 1.1  # Increase temperature to promote diversity\n",
        "        predicted_text = get_model_output(model, input_text, config, temperature=temperature, top_k=50)\n",
        "    else:\n",
        "        temperature = max(0.7, temperature * 0.95)  # Gradually decrease temperature back to a minimum\n",
        "\n",
        "    complete_text += \"‌‌‌‌‌ \" + predicted_text.strip()  # Ensure proper spacing\n",
        "    last_few_tokens.append(predicted_text.strip())\n",
        "\n",
        "    if len(last_few_tokens) > repetition_threshold:\n",
        "        last_few_tokens.pop(0)\n",
        "\n",
        "    # Truncate the input text if it exceeds the block size\n",
        "    input_text += \" \" + predicted_text.strip()\n",
        "    input_ids = tokenize_and_truncate_text(input_text, block_size)\n",
        "    input_text = tokenizer.decode(input_ids[0].tolist())  # Update input_text to truncated version\n",
        "\n",
        "    # Add a new line after [EOS]\n",
        "    if '[EOS]' and '[BOM]' in predicted_text:\n",
        "        complete_text += '\\n'\n",
        "        bom_count += 1\n",
        "\n",
        "# Print the whole output after the loop\n",
        "print(f\"Predicted Text:\\n{complete_text}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6664e2cfbe1440118cbe620f098a48b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b84199787324c18a9ef10173c481854",
              "IPY_MODEL_75e78b4ad63347d8887afd8508445180",
              "IPY_MODEL_386d141d54c44bed93425df5b8bab56a"
            ],
            "layout": "IPY_MODEL_db4ca1e4aa404d8c9254154b69319362"
          }
        },
        "3b84199787324c18a9ef10173c481854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffc578bc8360411dae1a505477fa862e",
            "placeholder": "​",
            "style": "IPY_MODEL_9fd996c3cca3405990cc251fc7b87332",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "75e78b4ad63347d8887afd8508445180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1e254232674161b8266d4fd0d16011",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_208629c1367546d5b725a93a38bf1a3a",
            "value": 350
          }
        },
        "386d141d54c44bed93425df5b8bab56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715bdbe1a9f8482db737d0ca9539f13f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fd9810ec60f4f0ca1842572f820df71",
            "value": " 350/350 [00:00&lt;00:00, 28.8kB/s]"
          }
        },
        "db4ca1e4aa404d8c9254154b69319362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffc578bc8360411dae1a505477fa862e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd996c3cca3405990cc251fc7b87332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1e254232674161b8266d4fd0d16011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "208629c1367546d5b725a93a38bf1a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715bdbe1a9f8482db737d0ca9539f13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd9810ec60f4f0ca1842572f820df71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fe67eafb2a94ffdaaa7844305f0634b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dafe1799aea4dec9008a215994c6bd5",
              "IPY_MODEL_bea0ccc19c364661b1e96626b6a2f19d",
              "IPY_MODEL_5d6f6a0999094d889c596256aab1f976"
            ],
            "layout": "IPY_MODEL_4c0db3c4591b46a9b6af09ec8d1ac6f0"
          }
        },
        "5dafe1799aea4dec9008a215994c6bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a6ad44e46147e0ad8a6680b812ef5c",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e2b2d131a94468936bcd7d03093c5e",
            "value": "config.json: 100%"
          }
        },
        "bea0ccc19c364661b1e96626b6a2f19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f973329ffea4c758ad8e79aefc06e49",
            "max": 1331,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99d3bf145f544cc8a904585fb31d2213",
            "value": 1331
          }
        },
        "5d6f6a0999094d889c596256aab1f976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80f684cf20c547eabefa5edb0d5b496e",
            "placeholder": "​",
            "style": "IPY_MODEL_3cc247ba80324e539d3a1a6dc5866ae3",
            "value": " 1.33k/1.33k [00:00&lt;00:00, 121kB/s]"
          }
        },
        "4c0db3c4591b46a9b6af09ec8d1ac6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a6ad44e46147e0ad8a6680b812ef5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e2b2d131a94468936bcd7d03093c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f973329ffea4c758ad8e79aefc06e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99d3bf145f544cc8a904585fb31d2213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80f684cf20c547eabefa5edb0d5b496e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc247ba80324e539d3a1a6dc5866ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcc03cebf4854db9a16ff13fee026cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1d63ec5b3c647aea453da4c37a0fc35",
              "IPY_MODEL_3fb6c408f497476e8e17ea32cd878275",
              "IPY_MODEL_173343cf9dfa4179b46b3af7e512f601"
            ],
            "layout": "IPY_MODEL_0ee1423d1b22456199af800eb53b73f0"
          }
        },
        "a1d63ec5b3c647aea453da4c37a0fc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6248b496ca24de0a235f2c12618f907",
            "placeholder": "​",
            "style": "IPY_MODEL_8392fc49e3ef42b9af1556284cec94fe",
            "value": "spiece.model: 100%"
          }
        },
        "3fb6c408f497476e8e17ea32cd878275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f65ce4e5954532abf7f7d63967acfb",
            "max": 537052,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2581473355464798a0c34b49d13b4295",
            "value": 537052
          }
        },
        "173343cf9dfa4179b46b3af7e512f601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f495acabdc46cb83623b136e16de38",
            "placeholder": "​",
            "style": "IPY_MODEL_bac3586499e949b58bb757e7b0b33f62",
            "value": " 537k/537k [00:00&lt;00:00, 9.87MB/s]"
          }
        },
        "0ee1423d1b22456199af800eb53b73f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6248b496ca24de0a235f2c12618f907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8392fc49e3ef42b9af1556284cec94fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f65ce4e5954532abf7f7d63967acfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2581473355464798a0c34b49d13b4295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f495acabdc46cb83623b136e16de38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac3586499e949b58bb757e7b0b33f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4089e885ff24840b4c369d12ca8f871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b733b000d85a4cb9b56b92201d406e15",
              "IPY_MODEL_63e703c2d4e848e78c0fa950ede6b1ae",
              "IPY_MODEL_a7361a404f534e148f15632a8474c061"
            ],
            "layout": "IPY_MODEL_955cfb2e343e4f4e8908c7fa4aa94902"
          }
        },
        "b733b000d85a4cb9b56b92201d406e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6305c9d5b8c240eaa00588e48988d2fa",
            "placeholder": "​",
            "style": "IPY_MODEL_d6d7bd48dcd740dd8b76dc830e915186",
            "value": "tokenizer.json: 100%"
          }
        },
        "63e703c2d4e848e78c0fa950ede6b1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d22d6778c3a942d988f6860f94d02e51",
            "max": 1127358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91cb2cc712d24364a20cdd76cfd97ece",
            "value": 1127358
          }
        },
        "a7361a404f534e148f15632a8474c061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e55a74807774b078d4f6d517f47aa62",
            "placeholder": "​",
            "style": "IPY_MODEL_5b94e4b316c9421581d6df2aa1ee46cd",
            "value": " 1.13M/1.13M [00:00&lt;00:00, 2.51MB/s]"
          }
        },
        "955cfb2e343e4f4e8908c7fa4aa94902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6305c9d5b8c240eaa00588e48988d2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d7bd48dcd740dd8b76dc830e915186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d22d6778c3a942d988f6860f94d02e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91cb2cc712d24364a20cdd76cfd97ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e55a74807774b078d4f6d517f47aa62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b94e4b316c9421581d6df2aa1ee46cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a5f41a528de4a249cb42379ddce014c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed86efbf9d6544fda8aa08296c53b095",
              "IPY_MODEL_64d56d1a6b634542ab5774b99c27d503",
              "IPY_MODEL_9cc1844ce7fe4d409abfde37be8a915e"
            ],
            "layout": "IPY_MODEL_82d3e26caeb5432e97a389f6d27f1ab2"
          }
        },
        "ed86efbf9d6544fda8aa08296c53b095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf352a365c34febb63ec37bf70d8bdd",
            "placeholder": "​",
            "style": "IPY_MODEL_619b5491c02a4222840032e6dc486672",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "64d56d1a6b634542ab5774b99c27d503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d65606cd1514695a9c5b0593db6c50a",
            "max": 399,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6e985ed91fe40f79d8a758c73dc344c",
            "value": 399
          }
        },
        "9cc1844ce7fe4d409abfde37be8a915e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9634ba56bf2d49c7a24fea1b297d633d",
            "placeholder": "​",
            "style": "IPY_MODEL_8c4dc91bd4604cba8be3ea7204e1df2d",
            "value": " 399/399 [00:00&lt;00:00, 34.0kB/s]"
          }
        },
        "82d3e26caeb5432e97a389f6d27f1ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf352a365c34febb63ec37bf70d8bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619b5491c02a4222840032e6dc486672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d65606cd1514695a9c5b0593db6c50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e985ed91fe40f79d8a758c73dc344c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9634ba56bf2d49c7a24fea1b297d633d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c4dc91bd4604cba8be3ea7204e1df2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}