{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682defe7-0a44-4f62-8923-2cefd5d8dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import inspect\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcf98bb-74b1-4006-8268-a5f5f5f35859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # Weight tying\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # Forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56af58c-f886-4ac7-bace-5f9cd929edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10\n",
      "Gradient Accumulation Steps: 1\n",
      "\n",
      "number of parameters: 1.79M\n",
      "Current optimizer param_groups: 1\n",
      "Saved optimizer param_groups: 2\n",
      "Parameter groups do not match.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.2393,  0.2208, -0.2748,  ...,  0.3717,  0.1468, -0.2151],\n",
      "        [ 0.2444,  0.2185, -0.2712,  ...,  0.3731,  0.1413, -0.2155],\n",
      "        [ 0.2391,  0.2176, -0.2745,  ...,  0.3695,  0.1424, -0.2093],\n",
      "        ...,\n",
      "        [ 0.2525,  0.2260, -0.2693,  ...,  0.3687,  0.1382, -0.2086],\n",
      "        [ 0.2399,  0.2196, -0.2659,  ...,  0.3831,  0.1443, -0.2125],\n",
      "        [-0.1025,  0.2832,  0.1171,  ..., -0.1070,  0.3294, -0.2589]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0765, -0.0312, -0.1295,  ..., -0.1225,  0.1956,  0.0979],\n",
      "        [ 0.0587, -0.0556, -0.0809,  ..., -0.1001,  0.1738,  0.0831],\n",
      "        [ 0.0564, -0.0677, -0.0654,  ..., -0.0867,  0.1504,  0.0752],\n",
      "        ...,\n",
      "        [-0.0960,  0.0220,  0.0866,  ..., -0.0168, -0.0657, -0.0099],\n",
      "        [-0.0957,  0.0156,  0.0862,  ..., -0.0122, -0.0623, -0.0073],\n",
      "        [-0.0871,  0.0016,  0.0803,  ..., -0.0192, -0.0604, -0.0031]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.2609, 1.3711, 1.3414, 1.3708, 1.1440, 1.3377, 1.2758, 1.1501, 1.2511,\n",
      "        1.1781, 1.3920, 1.3023, 1.3321, 1.3908, 1.2776, 1.3329, 1.2927, 1.2753,\n",
      "        1.2479, 1.3431, 1.3214, 1.2272, 1.3360, 1.2423, 1.3506, 1.3779, 1.3355,\n",
      "        1.1557, 1.4389, 1.3441, 1.3377, 1.2915, 1.2724, 1.3958, 1.3911, 1.3216,\n",
      "        1.4027, 1.4116, 1.4121, 1.2313, 1.4416, 1.2520, 1.2642, 1.2758, 1.3668,\n",
      "        1.2840, 1.3162, 1.3264, 1.3865, 1.2582, 1.3032, 1.3772, 1.2966, 1.2983,\n",
      "        1.2844, 1.3373, 1.3097, 1.3884, 1.3645, 1.2733, 1.3407, 1.2786, 1.2772,\n",
      "        1.3529], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-9.5343e-02,  1.5223e-02,  8.2490e-02, -5.4440e-03,  2.8959e-03,\n",
      "         1.0544e-02, -1.5298e-01,  1.0895e-01, -2.0536e-05,  1.7459e-01,\n",
      "         4.5556e-02, -3.1306e-02,  2.8102e-02,  8.6185e-03, -1.1254e-02,\n",
      "        -2.9056e-02,  6.0366e-02,  8.4793e-02,  1.3168e-01,  4.6076e-03,\n",
      "        -2.1192e-02,  2.5386e-03, -5.0243e-03, -4.2210e-03, -8.9100e-02,\n",
      "        -3.6939e-02,  3.3532e-04, -6.7588e-02,  4.2182e-02,  9.3186e-02,\n",
      "         3.6674e-02, -1.8195e-02, -5.2634e-02,  1.3603e-02, -3.9120e-02,\n",
      "         6.3374e-03,  1.1824e-01, -2.2637e-02, -1.9502e-02, -8.4460e-02,\n",
      "        -7.3984e-02,  3.7777e-02, -1.8852e-03, -7.1827e-03, -1.9112e-02,\n",
      "         4.9499e-02,  6.1799e-02, -7.5439e-02, -2.2871e-02, -5.7635e-02,\n",
      "        -2.8146e-02, -1.7195e-04,  5.2318e-02,  4.7788e-02,  1.0632e-01,\n",
      "        -8.1244e-02, -3.6334e-02, -1.3007e-01, -3.7885e-02, -1.8827e-03,\n",
      "         5.3710e-02, -1.5536e-02, -1.9356e-01, -6.5098e-03],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0333, -0.0506, -0.0284,  ...,  0.1115,  0.0323,  0.0543],\n",
      "        [ 0.1626,  0.0762,  0.0914,  ..., -0.0359,  0.2284, -0.0137],\n",
      "        [-0.1599,  0.0913, -0.0593,  ..., -0.1903, -0.0620,  0.1297],\n",
      "        ...,\n",
      "        [ 0.0224, -0.0048, -0.0735,  ...,  0.0283, -0.0021, -0.0020],\n",
      "        [-0.0005,  0.0067,  0.0056,  ..., -0.0484,  0.0145, -0.0219],\n",
      "        [-0.0579,  0.0026,  0.0244,  ..., -0.0151, -0.0267,  0.0026]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-6.6680e-01,  2.2593e-01,  4.4604e-01, -5.3527e-01, -4.1194e-01,\n",
      "        -4.8939e-01,  3.4234e-01, -3.9971e-01,  4.0581e-01, -6.7301e-01,\n",
      "         6.8847e-01, -5.8719e-01,  5.0284e-01,  3.7868e-01, -4.7566e-01,\n",
      "         2.3865e-01, -4.5302e-01, -2.9991e-02, -1.2712e-01, -8.5307e-02,\n",
      "        -5.0414e-01, -2.9516e-01,  2.6382e-01, -8.9077e-01, -3.8443e-01,\n",
      "         2.2545e-01, -3.9037e-01, -2.7124e-02, -3.2127e-01,  3.8981e-01,\n",
      "         6.9322e-01,  2.2780e-01,  3.5276e-02,  3.8446e-01, -6.4199e-01,\n",
      "         3.7014e-01,  1.4041e-01,  3.1033e-01, -6.6193e-01, -3.5034e-01,\n",
      "        -5.9707e-01, -1.3904e-01, -2.5771e-01, -1.4557e-02, -5.9089e-01,\n",
      "         3.9569e-01, -6.1744e-01, -3.4004e-01,  2.7736e-01, -2.9758e-02,\n",
      "        -2.5826e-01,  3.8624e-01,  2.4065e-01, -2.8773e-01, -4.3049e-01,\n",
      "        -1.1948e-01,  5.6606e-02,  3.7215e-01,  6.2474e-01, -3.6938e-01,\n",
      "         1.6185e-01,  1.0195e-01,  1.6529e-01,  6.5652e-01, -2.2705e-01,\n",
      "        -1.5376e-01,  4.1336e-01, -2.2121e-01, -8.3915e-02, -2.8599e-01,\n",
      "         4.0760e-02,  1.4100e-01,  3.0384e-02, -1.0927e-01,  2.5978e-01,\n",
      "        -2.6909e-01,  3.1931e-01,  1.3610e-01, -6.3338e-02, -5.0073e-03,\n",
      "        -1.9933e-01,  2.4736e-01, -3.5190e-02, -7.9596e-02, -2.4819e-01,\n",
      "        -8.4867e-02,  1.9175e-01, -2.6511e-01, -3.1015e-01, -1.1053e-01,\n",
      "        -1.0164e-01, -1.0665e-01, -1.3756e-01,  2.0341e-02,  1.7846e-01,\n",
      "         1.3310e-01, -1.5381e-01,  1.2723e-01, -1.3246e-01,  1.6387e-01,\n",
      "        -2.5776e-01,  1.6897e-01, -1.9145e-01, -1.7754e-01, -2.0682e-01,\n",
      "        -6.1034e-02, -2.3482e-01,  1.9426e-01, -2.1902e-01,  1.3895e-01,\n",
      "        -4.0910e-02,  1.9530e-02,  9.3816e-03, -8.8531e-03, -2.6112e-02,\n",
      "        -1.1353e-02,  2.5584e-02, -1.9994e-02, -6.0127e-02, -3.0030e-02,\n",
      "         1.5395e-02,  4.8421e-02,  4.9190e-02, -4.9330e-02, -1.3747e-02,\n",
      "        -2.7751e-02,  4.6003e-02,  6.2108e-02,  4.1234e-03,  6.7770e-03,\n",
      "         2.4499e-03,  3.3083e-02, -4.4678e-02, -1.2492e-02,  2.5005e-02,\n",
      "        -1.5894e-02, -5.6110e-03,  1.9117e-02,  1.8099e-02, -2.7772e-02,\n",
      "         1.8429e-02, -2.7344e-02,  3.4207e-02,  2.6510e-02,  1.5520e-02,\n",
      "         6.3860e-03,  8.1683e-03, -5.5853e-03, -6.3220e-03,  1.0126e-04,\n",
      "         2.6804e-03,  1.3207e-02, -2.6082e-02, -1.7619e-03, -1.0312e-02,\n",
      "         3.3818e-02,  1.6939e-02, -8.6283e-03, -2.1467e-02,  1.3884e-02,\n",
      "         1.4814e-02, -7.4280e-03,  1.2826e-03,  2.4596e-02, -5.3913e-03,\n",
      "        -2.5101e-02, -7.9910e-03,  1.6216e-02, -1.1628e-03, -5.5992e-02,\n",
      "         3.6404e-02,  7.2238e-03,  8.8279e-02,  4.1231e-02,  5.1784e-03,\n",
      "         1.3332e-03, -6.3660e-02, -3.9158e-02, -4.4947e-02,  6.2034e-02,\n",
      "        -1.0892e-02, -4.8843e-03,  2.0687e-02, -3.1622e-02, -9.1040e-03,\n",
      "         2.8906e-02,  2.5119e-02, -6.6798e-02,  2.9220e-02, -6.6670e-03,\n",
      "        -4.8058e-02,  3.9431e-02], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0518,  0.0630, -0.0113,  ...,  0.0020,  0.0009,  0.0497],\n",
      "        [ 0.0626, -0.0065, -0.0143,  ...,  0.0269, -0.0065, -0.0172],\n",
      "        [-0.0500,  0.0219,  0.0106,  ..., -0.0685, -0.0078,  0.0758],\n",
      "        ...,\n",
      "        [-0.0373,  0.0407, -0.0334,  ..., -0.0128,  0.0078,  0.0205],\n",
      "        [ 0.0494,  0.0462,  0.1001,  ...,  0.0052,  0.0519, -0.0608],\n",
      "        [ 0.0379, -0.0531,  0.0155,  ..., -0.0057,  0.0181, -0.0784]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 2.6075e-03,  7.6040e-04,  8.7870e-03, -6.2147e-03,  6.5358e-03,\n",
      "         7.2501e-03, -4.4917e-03,  1.3327e-02, -9.6439e-03,  2.3213e-02,\n",
      "         3.1660e-03,  3.6218e-03, -5.0514e-03,  9.5293e-04, -6.0959e-03,\n",
      "         1.4222e-03,  5.4781e-03, -5.9759e-03, -2.9936e-03,  1.0051e-03,\n",
      "        -6.8299e-03,  6.0367e-03, -4.6652e-04, -2.5402e-03,  3.3589e-03,\n",
      "        -1.9008e-04, -6.2109e-05, -1.0946e-02, -2.2779e-03, -2.9287e-03,\n",
      "        -6.5248e-03,  6.9625e-03,  3.7153e-03, -3.6709e-04,  7.4497e-03,\n",
      "        -4.3222e-03, -2.3954e-02,  1.5164e-03,  7.7606e-03, -4.3977e-03,\n",
      "         2.9673e-03, -6.0683e-04, -5.1130e-04,  6.4357e-03,  3.4295e-03,\n",
      "         5.1407e-03,  2.9902e-03, -3.4224e-03,  8.3146e-03,  5.8819e-03,\n",
      "        -6.4022e-03, -3.3671e-03, -5.5138e-03,  2.3363e-03, -2.4641e-04,\n",
      "        -5.3313e-03, -8.1695e-04,  3.6564e-03, -2.3883e-03,  2.8196e-04,\n",
      "        -3.9923e-03,  7.3930e-03, -4.1651e-03, -2.5859e-03],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.3557, 1.3124, 1.3955, 1.2819, 1.3206, 1.4858, 1.3578, 1.3386, 1.2914,\n",
      "        1.2854, 1.5907, 1.4224, 1.4405, 1.6264, 1.3204, 1.3143, 1.3510, 1.3125,\n",
      "        1.2824, 1.5261, 1.4340, 1.3160, 1.4561, 1.2806, 1.3396, 1.4292, 1.4265,\n",
      "        1.3024, 1.5351, 1.3900, 1.3566, 1.3308, 1.3781, 1.5754, 1.5338, 1.3374,\n",
      "        1.1615, 1.4821, 1.2492, 1.3613, 1.5363, 1.3802, 1.2207, 1.3219, 1.3764,\n",
      "        1.3360, 1.3425, 1.3104, 1.4164, 1.3171, 1.2452, 1.5233, 1.4393, 1.4495,\n",
      "        1.4675, 1.4296, 1.4546, 1.4159, 1.3916, 1.2746, 1.3933, 1.3653, 1.3333,\n",
      "        1.3753], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.0837,  0.0532,  0.1514,  0.0581,  0.1004,  0.2647, -0.0785,  0.2519,\n",
      "        -0.0709,  0.1992, -0.1148, -0.0919,  0.0825, -0.1977, -0.3214, -0.0704,\n",
      "         0.1515,  0.0172,  0.1143,  0.2219, -0.1638,  0.0439, -0.1291, -0.0625,\n",
      "         0.0479, -0.1230,  0.0074, -0.2092,  0.0295,  0.0533,  0.0933,  0.0024,\n",
      "        -0.1938, -0.0567, -0.0786, -0.0232,  0.0010, -0.0123,  0.0301, -0.0613,\n",
      "        -0.1503,  0.0661,  0.1264,  0.1391,  0.0571, -0.0101,  0.2079,  0.0506,\n",
      "         0.0471, -0.0039, -0.0169, -0.0149, -0.0717, -0.0173, -0.0271,  0.0324,\n",
      "         0.0202, -0.1550, -0.0642,  0.1492,  0.0974, -0.0483, -0.1187, -0.0075],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0388, -0.0755, -0.0089,  ...,  0.0162,  0.0029,  0.0152],\n",
      "        [-0.0110, -0.0295,  0.0665,  ..., -0.0060, -0.0184, -0.0168],\n",
      "        [-0.0118,  0.0014,  0.0539,  ...,  0.0116,  0.0260,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0559, -0.1003, -0.0799,  ...,  0.0056,  0.0151, -0.0651],\n",
      "        [ 0.0554,  0.0207,  0.0084,  ..., -0.0086,  0.0645, -0.0208],\n",
      "        [-0.0186, -0.0831,  0.0663,  ..., -0.0319,  0.0315,  0.0203]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.0808, -0.0730, -0.0847, -0.0076,  0.0297, -0.0202, -0.0554, -0.0018,\n",
      "        -0.0341, -0.0738, -0.0538, -0.0847, -0.0696, -0.0250, -0.0278, -0.0584,\n",
      "        -0.0196,  0.0212, -0.0218, -0.0237, -0.0418, -0.0727, -0.0908, -0.0400,\n",
      "        -0.0649, -0.0690, -0.0076, -0.0592, -0.0169, -0.0627,  0.0085, -0.0466,\n",
      "        -0.0432, -0.0029, -0.0796, -0.0296, -0.0512, -0.0732, -0.0245, -0.0908,\n",
      "        -0.0717, -0.0209, -0.0035, -0.0645, -0.0389, -0.0170, -0.0782, -0.0093,\n",
      "        -0.0488, -0.0192, -0.0478, -0.0410, -0.1084, -0.0185, -0.0255, -0.0670,\n",
      "        -0.0298, -0.0513, -0.0676, -0.0210, -0.0124, -0.1205, -0.1024, -0.0375,\n",
      "        -0.0235, -0.1072, -0.0560,  0.0735, -0.0442,  0.0025, -0.0336, -0.0904,\n",
      "        -0.0742, -0.0264, -0.0316,  0.0004, -0.0322, -0.0553, -0.0790, -0.0354,\n",
      "        -0.0652, -0.0762, -0.0393, -0.0829, -0.0570, -0.0578, -0.1039, -0.0158,\n",
      "        -0.0315, -0.0678, -0.0749, -0.0774, -0.0424, -0.0653, -0.0369,  0.0009,\n",
      "        -0.0176, -0.0847, -0.0359, -0.0590, -0.0066, -0.1042, -0.0443, -0.0526,\n",
      "        -0.0342, -0.0245, -0.0130, -0.0204, -0.0373, -0.1095, -0.0247, -0.0530,\n",
      "        -0.0535, -0.0306, -0.0537, -0.0599, -0.1126, -0.0236, -0.0068, -0.0562,\n",
      "        -0.0144, -0.0443, -0.0273, -0.0413, -0.0482, -0.0834, -0.0395, -0.0320,\n",
      "        -0.0036, -0.0112, -0.0617, -0.0393,  0.0180, -0.1034, -0.0379, -0.0505,\n",
      "        -0.0382,  0.0113, -0.0675, -0.0605, -0.0175, -0.0579, -0.0725, -0.0860,\n",
      "        -0.0074, -0.0666, -0.0426, -0.0405, -0.0825, -0.0071, -0.0791, -0.0252,\n",
      "        -0.0486, -0.0673, -0.0201, -0.0793, -0.0698, -0.0471, -0.0589, -0.0346,\n",
      "        -0.0495, -0.0658, -0.0520, -0.0741, -0.1109, -0.0275, -0.0432, -0.0644,\n",
      "        -0.0446, -0.0354, -0.0871, -0.0423, -0.0456, -0.0467, -0.0361, -0.0644,\n",
      "        -0.0504, -0.0569, -0.0328, -0.0254, -0.0766, -0.1020, -0.0419, -0.0287,\n",
      "        -0.0283, -0.0136, -0.0495, -0.0091, -0.0076,  0.0219, -0.0312, -0.0763,\n",
      "        -0.0046, -0.0415, -0.0566, -0.0002, -0.0294, -0.0474, -0.0401, -0.0770,\n",
      "        -0.0161, -0.0895, -0.0353, -0.0929, -0.0410, -0.0638, -0.0207, -0.0224,\n",
      "        -0.0041, -0.0489, -0.0723, -0.0838, -0.0717, -0.0380,  0.0337, -0.0330,\n",
      "        -0.0222, -0.0053, -0.0881, -0.0589, -0.0014, -0.0372, -0.0356, -0.0924,\n",
      "        -0.0101, -0.0019, -0.0069, -0.0279, -0.0696, -0.0719, -0.0836, -0.0582,\n",
      "        -0.0106, -0.0126,  0.0135, -0.0502, -0.0295, -0.0899, -0.0804,  0.0367,\n",
      "        -0.0547, -0.0319, -0.0679, -0.0170, -0.0526, -0.0546, -0.0264, -0.0838,\n",
      "        -0.0215, -0.0547, -0.0265, -0.0521, -0.0385, -0.0462,  0.0055, -0.0608],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0209, -0.0289, -0.0218,  ..., -0.0443,  0.0722,  0.0381],\n",
      "        [ 0.0155, -0.0070, -0.0045,  ..., -0.0006,  0.0082, -0.0117],\n",
      "        [-0.0087, -0.0190, -0.0048,  ...,  0.0104, -0.0174, -0.0211],\n",
      "        ...,\n",
      "        [-0.0180, -0.0129,  0.0073,  ...,  0.0078,  0.0052,  0.0384],\n",
      "        [-0.0019,  0.0153,  0.0182,  ..., -0.0009,  0.0175,  0.0351],\n",
      "        [-0.0099, -0.0018, -0.0061,  ..., -0.0585, -0.0060, -0.0028]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-1.9351e-02, -1.4418e-02,  8.1068e-03,  6.0116e-03, -7.8046e-03,\n",
      "         4.5629e-04, -1.6346e-02, -3.5932e-02, -5.6741e-03, -5.5219e-02,\n",
      "         1.2655e-03, -5.2240e-03,  1.7102e-02,  5.9065e-03,  4.6288e-03,\n",
      "         7.4987e-03,  4.8793e-03, -1.7224e-02,  9.2392e-03, -7.1300e-03,\n",
      "         1.0915e-02,  5.0936e-03, -1.2064e-03, -8.0621e-04,  3.4535e-04,\n",
      "        -2.0102e-03, -2.8189e-03,  8.4649e-03, -2.4995e-03,  1.3528e-03,\n",
      "        -2.6556e-03,  1.2761e-02, -3.3392e-04,  4.3927e-03, -3.8163e-03,\n",
      "         5.6114e-03, -9.4946e-03,  9.1305e-04, -3.5267e-04,  1.5387e-02,\n",
      "        -1.6577e-03, -1.7398e-03, -3.7726e-06,  1.0664e-03,  1.8004e-02,\n",
      "         1.1879e-02, -6.5874e-03, -2.0109e-02,  9.7135e-03,  6.2650e-03,\n",
      "         1.1723e-02, -1.0579e-02,  1.7377e-02, -5.5024e-02, -9.8801e-03,\n",
      "        -9.1350e-03, -1.5247e-04,  2.1686e-03,  4.2754e-02, -2.4087e-03,\n",
      "        -9.9487e-04,  7.5268e-03, -1.5922e-02, -2.4176e-03],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.1239, 1.1109, 1.1697, 1.0795, 1.0995, 1.1757, 1.0684, 1.0081, 1.1335,\n",
      "        1.0246, 1.1713, 1.2254, 1.1895, 1.2152, 1.1494, 1.1879, 1.1051, 1.1483,\n",
      "        1.1901, 1.1574, 1.0914, 1.0664, 1.0982, 1.1565, 1.1365, 1.2208, 1.2050,\n",
      "        1.1135, 1.2571, 1.1343, 1.1413, 1.1235, 1.1018, 1.2153, 1.2129, 1.1138,\n",
      "        1.4644, 1.2415, 1.2632, 1.0222, 1.1331, 1.1531, 1.1177, 1.1336, 1.0184,\n",
      "        1.1633, 1.0998, 1.1114, 1.1420, 1.1420, 1.1254, 1.1325, 1.0998, 1.0917,\n",
      "        1.0937, 1.1873, 1.1391, 1.1612, 1.0841, 1.0939, 1.1623, 1.0616, 1.0858,\n",
      "        1.1615], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.1681, -0.1596,  0.2164, -0.0344,  0.2826,  0.2552, -0.0529, -0.1957,\n",
      "        -0.1183, -0.0402, -0.0292, -0.0585,  0.1627, -0.0180, -0.1492,  0.0646,\n",
      "         0.0540, -0.1270,  0.1315,  0.1982,  0.0460,  0.1138,  0.1702, -0.0545,\n",
      "         0.1725,  0.0632,  0.0979, -0.2686,  0.2305,  0.0700,  0.1149,  0.1900,\n",
      "        -0.0386, -0.0410, -0.2135, -0.0566,  0.0552,  0.0581, -0.0407,  0.0498,\n",
      "         0.0525,  0.1486,  0.1293,  0.2143,  0.4434, -0.0646, -0.0787, -0.0864,\n",
      "         0.2545,  0.1293,  0.0878, -0.0978,  0.0648, -0.0734,  0.0783,  0.1768,\n",
      "        -0.0587,  0.0304,  0.0022,  0.1203,  0.1904,  0.1603, -0.0777,  0.1265],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0206,  0.0328, -0.0189,  ...,  0.0513, -0.0200,  0.0272],\n",
      "        [ 0.0929, -0.0678,  0.0410,  ...,  0.0162, -0.0372,  0.0068],\n",
      "        [ 0.0133, -0.0506,  0.0135,  ...,  0.0088, -0.0114, -0.0472],\n",
      "        ...,\n",
      "        [ 0.0361,  0.0201, -0.0379,  ...,  0.0747,  0.0168,  0.0247],\n",
      "        [ 0.0131,  0.0182, -0.0507,  ..., -0.0160, -0.0079,  0.0085],\n",
      "        [ 0.0028,  0.0284,  0.0402,  ..., -0.0373, -0.0403,  0.0230]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-3.5039e-01,  2.8826e-01, -3.1498e-02, -2.7995e-01, -2.6509e-01,\n",
      "        -1.6599e-02, -6.0832e-02,  2.7904e-01,  3.1870e-01, -2.9241e-01,\n",
      "        -3.1763e-01, -2.3927e-01, -2.8707e-01,  1.7230e-01,  3.1643e-01,\n",
      "         3.1433e-01, -4.6209e-01, -3.8362e-01, -1.0355e-01,  3.3887e-01,\n",
      "        -5.1513e-01,  1.0788e-01,  3.7842e-01,  2.6372e-01,  6.2589e-02,\n",
      "         1.0819e-01,  2.0925e-01, -1.1627e-01, -2.2146e-01,  4.7024e-01,\n",
      "        -6.3258e-02,  2.1679e-01, -4.7303e-01, -2.4995e-01,  9.7814e-02,\n",
      "        -4.1114e-01, -1.9000e-01,  1.7752e-01, -4.8794e-01, -3.7873e-01,\n",
      "        -3.8291e-01, -1.9690e-01,  5.3904e-01,  3.9617e-01, -2.2667e-01,\n",
      "         1.7943e-01,  2.0788e-01,  3.5708e-01,  3.4251e-01, -1.9963e-01,\n",
      "        -4.6015e-01,  4.6732e-01,  2.5825e-01, -3.3011e-01, -5.0047e-01,\n",
      "        -6.6259e-02,  4.7839e-01,  4.9928e-01,  1.5193e-01, -2.7881e-01,\n",
      "        -3.6273e-01, -2.2465e-01,  3.0910e-01,  3.6272e-02, -1.0977e-01,\n",
      "         1.1721e-01,  8.1334e-02, -1.0413e-01, -9.2386e-02, -8.0047e-02,\n",
      "         6.2375e-02,  7.9357e-02,  1.0903e-01, -9.0706e-02, -1.2725e-01,\n",
      "        -7.2755e-02, -8.1666e-02,  7.2546e-02,  1.1637e-01,  1.0678e-01,\n",
      "        -2.0123e-02, -2.1479e-02,  1.7193e-02, -6.4635e-03, -1.5761e-02,\n",
      "         2.8548e-02, -1.8343e-03,  3.1440e-02, -2.3292e-02,  2.7324e-02,\n",
      "         3.5685e-02, -2.7802e-02, -2.3648e-02,  1.5329e-02,  2.0236e-02,\n",
      "         1.8518e-02, -3.4211e-02, -2.5361e-02, -3.5707e-02, -1.6127e-02,\n",
      "         1.7212e-02, -2.8016e-02, -4.5406e-02, -1.7081e-02, -1.9536e-02,\n",
      "         2.7802e-02,  5.6908e-02,  1.5694e-02,  2.2614e-02, -3.0503e-02,\n",
      "        -2.5089e-02,  1.6002e-02,  3.9021e-02,  5.0596e-03, -4.5414e-02,\n",
      "         1.2387e-03,  2.7748e-02, -2.8934e-02, -5.4270e-02, -4.4063e-03,\n",
      "         2.5539e-02, -7.0780e-04, -5.9958e-03, -9.3245e-03, -2.6255e-02,\n",
      "         3.2689e-02, -2.4503e-03,  4.9128e-02,  1.0347e-01,  1.1184e-01,\n",
      "         1.0397e-01, -1.1387e-01,  1.5358e-01, -8.7338e-02, -1.2347e-01,\n",
      "        -4.2476e-02, -7.9072e-02, -1.3573e-01,  1.0433e-01, -6.4247e-02,\n",
      "         3.2661e-02,  2.8658e-02, -1.0805e-01,  8.8226e-02,  4.8218e-02,\n",
      "         8.3900e-02,  3.6316e-02,  8.6047e-02, -2.7762e-02,  4.0339e-02,\n",
      "         4.5502e-02,  3.9675e-02,  1.8568e-02,  6.4994e-02,  1.8795e-02,\n",
      "        -5.3905e-02, -9.0949e-02, -1.2376e-01,  7.1508e-02, -8.0257e-02,\n",
      "        -7.1004e-02,  3.8871e-02,  5.9077e-02,  5.3328e-03,  3.4874e-02,\n",
      "        -2.0008e-02, -5.8074e-02, -1.8046e-02,  4.4245e-02,  4.2685e-02,\n",
      "         2.1236e-02, -6.8078e-02, -4.0751e-02,  9.4702e-03, -5.5624e-02,\n",
      "        -5.2568e-02, -2.7586e-02, -1.8080e-02,  5.2003e-03, -3.5784e-02,\n",
      "         1.6727e-04,  1.5862e-02, -2.3720e-02,  8.8211e-03, -8.4309e-03,\n",
      "         1.6074e-02,  7.3952e-03,  1.8044e-02, -4.4179e-02,  1.8580e-02,\n",
      "         1.8628e-02, -1.1175e-02], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0827, -0.0420, -0.0815,  ...,  0.0497,  0.0933, -0.0634],\n",
      "        [ 0.0351,  0.0368,  0.0475,  ..., -0.0155,  0.0141,  0.0750],\n",
      "        [ 0.0048, -0.0085,  0.0214,  ..., -0.0283,  0.0011,  0.0256],\n",
      "        ...,\n",
      "        [-0.0150, -0.0094, -0.0274,  ..., -0.0282, -0.0088, -0.0221],\n",
      "        [ 0.0277,  0.0217, -0.0252,  ..., -0.1082,  0.0624,  0.0014],\n",
      "        [-0.0054,  0.0218, -0.0073,  ...,  0.0657, -0.0226, -0.0087]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 0.0341, -0.0187, -0.0055,  0.0053, -0.0084, -0.0038,  0.0033,  0.0243,\n",
      "        -0.0159,  0.0566, -0.0036,  0.0075, -0.0092,  0.0043, -0.0104,  0.0089,\n",
      "         0.0049,  0.0110, -0.0014, -0.0053,  0.0029, -0.0092, -0.0080, -0.0140,\n",
      "         0.0252,  0.0014,  0.0032,  0.0004,  0.0013,  0.0056,  0.0076, -0.0153,\n",
      "        -0.0104,  0.0015,  0.0210, -0.0049, -0.0185,  0.0017,  0.0077,  0.0025,\n",
      "         0.0031, -0.0079,  0.0164, -0.0123, -0.0212, -0.0161,  0.0040, -0.0082,\n",
      "        -0.0339, -0.0129,  0.0011,  0.0066, -0.0068,  0.0653,  0.0171, -0.0198,\n",
      "         0.0032,  0.0135, -0.0049,  0.0124, -0.0039,  0.0055,  0.0101, -0.0017],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.2382, 1.3399, 1.3996, 1.4223, 1.3248, 1.3643, 1.4178, 1.1966, 1.2635,\n",
      "        1.0534, 1.6301, 1.3825, 1.4270, 1.6427, 1.3288, 1.4568, 1.2508, 1.3237,\n",
      "        1.3790, 1.5521, 1.4405, 1.2831, 1.4386, 1.3856, 1.4175, 1.4865, 1.5531,\n",
      "        1.4243, 1.5669, 1.5038, 1.4367, 1.3620, 1.2893, 1.5629, 1.3564, 1.3778,\n",
      "        1.0959, 1.5681, 1.4289, 1.1042, 1.5048, 1.3101, 1.3684, 1.3393, 1.2757,\n",
      "        1.3405, 1.3453, 1.4663, 1.3914, 1.4252, 1.3761, 1.5663, 1.4659, 1.2414,\n",
      "        1.4223, 1.4339, 1.4607, 1.4766, 1.2893, 1.2653, 1.4987, 1.4262, 1.1778,\n",
      "        1.3383], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.1571, -0.1093,  0.0454,  0.0086,  0.0612,  0.1389, -0.0320, -0.0381,\n",
      "        -0.0307, -0.0009,  0.0269, -0.1582, -0.0100,  0.0139, -0.0504, -0.0766,\n",
      "         0.0677,  0.0598, -0.0034,  0.0795, -0.0114, -0.0013,  0.0174, -0.0410,\n",
      "         0.1214,  0.0623,  0.0724, -0.2266,  0.0566, -0.0975,  0.1112,  0.0118,\n",
      "        -0.0211,  0.0406, -0.0681,  0.0025,  0.0937, -0.0125, -0.1614,  0.0022,\n",
      "        -0.1220, -0.0011,  0.0862,  0.0786,  0.1706,  0.0047,  0.0943, -0.1269,\n",
      "         0.1814,  0.1205,  0.0631, -0.0784, -0.0175, -0.0082,  0.1488, -0.0300,\n",
      "         0.0013, -0.0516, -0.0575, -0.0037,  0.0875, -0.0204, -0.0087, -0.0172],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0162, -0.0071,  0.0090,  ...,  0.0095,  0.0343, -0.0741],\n",
      "        [ 0.0613, -0.0138, -0.0496,  ...,  0.0349,  0.0453, -0.0284],\n",
      "        [ 0.0189,  0.1091, -0.0337,  ..., -0.0839,  0.0145,  0.0013],\n",
      "        ...,\n",
      "        [-0.0154, -0.0144, -0.0109,  ...,  0.0594,  0.0853,  0.0698],\n",
      "        [-0.0059, -0.0122,  0.1017,  ...,  0.0474, -0.0493, -0.0117],\n",
      "        [ 0.0047, -0.0269, -0.0247,  ..., -0.0534,  0.0580, -0.0036]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.0616,  0.0042, -0.0285, -0.0903, -0.0558, -0.0483, -0.0350, -0.0966,\n",
      "        -0.0242, -0.0194, -0.0676, -0.0872, -0.0743, -0.0442,  0.0288, -0.0418,\n",
      "        -0.1111, -0.0173, -0.0524, -0.0075, -0.0320, -0.0277, -0.0217, -0.0244,\n",
      "        -0.0549, -0.0715, -0.0153, -0.0568, -0.0386, -0.0610, -0.1090, -0.0713,\n",
      "        -0.0276, -0.0251, -0.0177,  0.0083, -0.0327, -0.0690,  0.0061, -0.0367,\n",
      "        -0.0884, -0.0613, -0.0275, -0.0848, -0.0721, -0.0025, -0.0814, -0.0434,\n",
      "        -0.0547, -0.0278, -0.0394, -0.0265, -0.0509, -0.0400,  0.0078, -0.0280,\n",
      "        -0.0121,  0.0034,  0.0373, -0.0321, -0.0236, -0.0375, -0.0238, -0.0381,\n",
      "        -0.0503, -0.0558, -0.0795, -0.0265, -0.0351, -0.0294, -0.0859, -0.0596,\n",
      "        -0.0763, -0.0091, -0.0830, -0.0121, -0.0324, -0.0220, -0.0348, -0.0460,\n",
      "        -0.0192, -0.0117, -0.0603, -0.0285, -0.0426, -0.0029, -0.0157, -0.0994,\n",
      "        -0.0199, -0.0040, -0.0727, -0.0047, -0.0222, -0.0560, -0.0562, -0.0281,\n",
      "        -0.0760, -0.0053,  0.0075, -0.0040, -0.0298, -0.0595, -0.0602, -0.0106,\n",
      "        -0.0955, -0.0369, -0.0349, -0.0631, -0.0472, -0.0364, -0.0616,  0.0045,\n",
      "        -0.0482, -0.0541, -0.0333, -0.0101, -0.0159, -0.0764, -0.0515, -0.0256,\n",
      "        -0.0874, -0.0371, -0.0324, -0.0130, -0.0470,  0.0024, -0.0936, -0.0004,\n",
      "         0.0214, -0.0360, -0.0479, -0.0655, -0.0078, -0.0516, -0.0853, -0.0308,\n",
      "        -0.0387, -0.0814, -0.0828, -0.0177, -0.0704, -0.0377, -0.0383, -0.0586,\n",
      "        -0.0568, -0.0606, -0.0515, -0.0522, -0.0899, -0.0405, -0.0730,  0.0140,\n",
      "         0.0049, -0.0436, -0.0672, -0.0223, -0.0392, -0.0231, -0.0651, -0.0268,\n",
      "        -0.0182, -0.0749, -0.0507, -0.0306, -0.0317, -0.0685, -0.0712, -0.0456,\n",
      "        -0.0460, -0.0482, -0.0942, -0.0337, -0.0640, -0.0786, -0.0309, -0.0352,\n",
      "        -0.0638, -0.0726, -0.0543,  0.0144, -0.0778,  0.0234, -0.0754, -0.0581,\n",
      "         0.0076, -0.0734, -0.0262, -0.0595, -0.0486, -0.0471, -0.0691, -0.0622,\n",
      "        -0.0912,  0.0150, -0.0399, -0.0598, -0.0461, -0.0263, -0.0137, -0.0269,\n",
      "        -0.0129, -0.0390, -0.0557, -0.0887, -0.0206, -0.0676, -0.0017, -0.0619,\n",
      "        -0.0504, -0.0354, -0.0507, -0.0295, -0.0902, -0.0290, -0.0081, -0.0485,\n",
      "        -0.0286, -0.0045,  0.0201, -0.0585, -0.0154, -0.0679, -0.0789, -0.0136,\n",
      "         0.0249,  0.0015, -0.0597, -0.0583, -0.0685, -0.0634, -0.0523, -0.0415,\n",
      "         0.0059, -0.0659, -0.0537,  0.0117, -0.0067, -0.0321, -0.0413, -0.0381,\n",
      "        -0.0552, -0.0293, -0.0854, -0.0731, -0.0343, -0.1113, -0.0589,  0.0039,\n",
      "        -0.0423, -0.0758, -0.0749, -0.0200, -0.0172, -0.0047, -0.0320, -0.0341],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0211,  0.0049, -0.0089,  ..., -0.0135, -0.0399,  0.0513],\n",
      "        [-0.0144,  0.0171, -0.0044,  ..., -0.0396, -0.0203, -0.0099],\n",
      "        [ 0.0223, -0.0313,  0.0025,  ..., -0.0564,  0.0394, -0.0218],\n",
      "        ...,\n",
      "        [ 0.0031,  0.0623,  0.0300,  ...,  0.0404,  0.0704, -0.0473],\n",
      "        [ 0.0002,  0.0011,  0.0028,  ..., -0.0428,  0.0019,  0.0415],\n",
      "        [-0.0153,  0.0058,  0.0205,  ...,  0.0572,  0.0181, -0.0174]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 0.0005, -0.0170, -0.0025,  0.0063, -0.0009,  0.0054,  0.0099, -0.0319,\n",
      "        -0.0388, -0.0350,  0.0089, -0.0064, -0.0002,  0.0058, -0.0223,  0.0083,\n",
      "        -0.0073, -0.0170,  0.0035,  0.0004, -0.0038, -0.0019, -0.0031, -0.0033,\n",
      "         0.0222, -0.0024,  0.0023, -0.0009, -0.0016,  0.0081,  0.0088,  0.0134,\n",
      "         0.0016,  0.0001, -0.0157, -0.0049, -0.0185,  0.0061,  0.0015,  0.0377,\n",
      "         0.0055,  0.0070,  0.0053, -0.0043,  0.0251,  0.0012,  0.0066, -0.0031,\n",
      "         0.0102,  0.0012, -0.0158, -0.0025,  0.0045, -0.0371, -0.0286,  0.0047,\n",
      "         0.0019,  0.0061,  0.0139,  0.0187, -0.0111, -0.0068, -0.0003,  0.0066],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.1418, 1.2577, 1.2079, 1.2013, 1.1640, 1.2356, 1.1284, 1.1073, 1.1755,\n",
      "        1.0959, 1.3123, 1.2327, 1.2846, 1.2749, 1.2039, 1.2626, 1.0960, 1.1845,\n",
      "        1.1748, 1.2853, 1.2083, 1.1770, 1.2262, 1.2012, 1.2724, 1.3078, 1.2850,\n",
      "        1.1400, 1.3861, 1.2908, 1.2356, 1.2262, 1.1387, 1.3366, 1.2412, 1.1696,\n",
      "        1.4474, 1.3585, 1.3852, 1.1316, 1.2964, 1.1955, 1.2182, 1.1792, 1.1606,\n",
      "        1.2185, 1.2059, 1.1998, 1.2324, 1.2162, 1.1715, 1.3101, 1.1548, 1.1554,\n",
      "        1.2307, 1.2684, 1.2950, 1.1976, 1.1960, 1.1905, 1.2898, 1.1933, 1.1155,\n",
      "        1.1628], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.2515, -0.2915,  0.2469, -0.0252,  0.2679,  0.2942,  0.0285, -0.4865,\n",
      "        -0.2100, -0.0656, -0.1561, -0.0139,  0.1314, -0.1851, -0.1755,  0.2217,\n",
      "        -0.3162, -0.1662,  0.2112,  0.1686, -0.0669, -0.0367, -0.0507, -0.0904,\n",
      "         0.2633, -0.2156, -0.0850, -0.1151,  0.1553,  0.1527,  0.2119,  0.2165,\n",
      "        -0.1381, -0.0169, -0.2432, -0.0176,  0.0113,  0.0031,  0.0800,  0.2021,\n",
      "        -0.1089,  0.1405,  0.2738,  0.3358,  0.3295, -0.1334,  0.1179,  0.0287,\n",
      "         0.2091, -0.0147, -0.1693,  0.0624, -0.0428, -0.1840, -0.1076,  0.2069,\n",
      "        -0.0486, -0.0944, -0.0105,  0.1939,  0.1481, -0.0751, -0.1642,  0.1614],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0693,  0.0043, -0.0175,  ...,  0.0785, -0.1238, -0.0713],\n",
      "        [-0.0786,  0.1712, -0.0633,  ..., -0.1355, -0.0092, -0.0443],\n",
      "        [ 0.0684, -0.1762,  0.0645,  ..., -0.1110,  0.0247, -0.1158],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0307,  0.0011,  ..., -0.0172,  0.0270, -0.0344],\n",
      "        [-0.0226, -0.0117, -0.0384,  ...,  0.0307, -0.0290, -0.0098],\n",
      "        [ 0.0203,  0.0402,  0.0078,  ...,  0.0659, -0.0113,  0.0426]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 1.3561e-01,  1.0516e-01,  2.1304e-01,  8.7498e-02,  2.4558e-01,\n",
      "         2.9492e-01,  3.6765e-01,  4.2775e-01, -3.4122e-01, -2.8972e-01,\n",
      "        -1.3989e-01, -2.6499e-01,  1.8259e-01, -1.0408e-01,  2.5981e-01,\n",
      "        -3.0988e-01, -3.7203e-01, -1.9766e-01, -3.5933e-01,  7.3480e-01,\n",
      "        -3.3891e-01,  4.8882e-01,  3.9185e-01, -4.3136e-01, -4.2160e-01,\n",
      "         7.7142e-03, -3.4512e-01, -3.5019e-01, -3.2229e-01, -3.7516e-01,\n",
      "        -3.4680e-01,  4.5670e-01,  3.1429e-01, -3.1518e-01,  1.0348e+00,\n",
      "         1.0256e+00, -4.8818e-01, -4.9040e-01,  9.8607e-01,  9.2440e-01,\n",
      "         6.7958e-01, -3.7409e-01, -6.3609e-01, -5.0065e-01, -2.8230e-01,\n",
      "        -8.8750e-01,  9.7527e-01, -6.9987e-01, -2.8736e-01, -2.1536e-01,\n",
      "         2.9770e-01,  2.5781e-01, -3.6932e-01, -2.8979e-01,  2.4966e-01,\n",
      "         1.6946e-01,  2.4469e-01, -3.6591e-01, -4.7427e-01, -3.2473e-01,\n",
      "        -2.6266e-01, -4.0422e-01, -2.9900e-01,  1.9483e-01,  9.4840e-02,\n",
      "        -8.7217e-03,  6.7206e-02,  6.6545e-02, -1.2153e-02,  1.1110e-01,\n",
      "         8.1772e-02, -4.9947e-02, -1.6351e-01, -6.9263e-02, -8.0640e-02,\n",
      "        -8.1806e-02,  1.5115e-02, -9.4989e-05,  2.1274e-02, -5.6134e-02,\n",
      "        -4.0100e-02, -5.5399e-02, -2.1436e-02,  5.5995e-02, -2.7277e-02,\n",
      "         2.2676e-02,  3.1254e-02, -2.7454e-02, -2.7130e-02, -4.9249e-02,\n",
      "        -3.6809e-02, -3.0968e-02, -3.0418e-02, -1.7113e-02, -3.5515e-02,\n",
      "         5.6965e-02, -5.0621e-02,  2.1384e-02, -8.3372e-02, -5.8620e-02,\n",
      "        -3.3143e-02,  1.8200e-02, -2.7956e-02, -8.4075e-02, -3.0741e-02,\n",
      "         5.9371e-03,  2.3998e-02,  1.1315e-02,  1.3362e-02,  8.2875e-02,\n",
      "        -1.5029e-02,  4.2971e-02,  3.0815e-02,  2.9774e-02, -3.3992e-02,\n",
      "        -1.6408e-02, -4.1831e-03,  1.3180e-02,  2.0011e-02,  2.2318e-02,\n",
      "        -1.4697e-02, -6.5699e-03, -8.3641e-03,  4.2180e-02,  2.6732e-02,\n",
      "        -1.6786e-02,  1.2747e-02,  1.0439e-02,  5.1305e-03, -1.3525e-02,\n",
      "        -2.6238e-03, -1.2826e-02, -8.4292e-03,  1.8769e-02,  2.6603e-02,\n",
      "         2.9906e-02,  2.5481e-03,  5.8235e-04, -1.6824e-02, -8.1177e-03,\n",
      "        -1.9382e-02,  1.3562e-02, -2.2585e-02,  8.0363e-03, -3.8428e-02,\n",
      "         3.9951e-02, -4.9657e-02,  7.9849e-02,  9.6262e-02, -9.6724e-02,\n",
      "         2.3928e-02, -2.0145e-02, -6.4779e-03, -7.4291e-02, -7.8706e-02,\n",
      "        -9.9925e-02,  1.7967e-02, -2.4137e-02, -2.8713e-02,  6.6413e-02,\n",
      "         3.3016e-02, -2.9443e-02,  1.2766e-02, -6.0922e-02, -4.3282e-02,\n",
      "        -3.1278e-03, -2.4921e-03, -2.0336e-02,  1.7239e-02,  2.3461e-02,\n",
      "         1.8079e-02,  1.4377e-02, -2.2641e-02,  7.9076e-02,  1.1400e-02,\n",
      "         5.0615e-02,  2.3483e-03, -3.7726e-03,  7.4820e-03,  2.0541e-02,\n",
      "         2.5217e-03, -5.5675e-03,  2.4569e-02, -6.0931e-03,  1.2004e-02,\n",
      "         8.2219e-04,  1.8958e-02,  2.0659e-02, -2.4161e-02, -2.6264e-02,\n",
      "        -1.6337e-03,  1.9096e-02], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-0.0746, -0.0810,  0.0554,  ...,  0.0007,  0.0415, -0.0335],\n",
      "        [-0.0882, -0.0247, -0.0113,  ...,  0.0734,  0.0069,  0.1327],\n",
      "        [-0.0623,  0.1139, -0.0315,  ...,  0.1082, -0.0300, -0.0742],\n",
      "        ...,\n",
      "        [-0.0035,  0.0204,  0.0196,  ...,  0.1124, -0.0251,  0.0132],\n",
      "        [-0.0154, -0.0360, -0.0622,  ..., -0.0952,  0.0175, -0.0597],\n",
      "        [-0.0295,  0.0451,  0.0239,  ...,  0.0117,  0.0533, -0.0093]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 0.0021,  0.0186,  0.0023, -0.0019, -0.0155,  0.0069, -0.0154,  0.0122,\n",
      "        -0.0150, -0.0179,  0.0011,  0.0058,  0.0045,  0.0083,  0.0007,  0.0050,\n",
      "         0.0088, -0.0142,  0.0102, -0.0080,  0.0089, -0.0017, -0.0042,  0.0035,\n",
      "         0.0082,  0.0399,  0.0045,  0.0045,  0.0013,  0.0127,  0.0062,  0.0013,\n",
      "        -0.0027,  0.0010, -0.0234, -0.0175, -0.0160, -0.0018,  0.0008, -0.0154,\n",
      "         0.0009,  0.0006, -0.0159, -0.0146,  0.0110, -0.0138,  0.0032, -0.0180,\n",
      "         0.0011, -0.0090, -0.0010,  0.0031,  0.0109,  0.0291,  0.0376, -0.0154,\n",
      "        -0.0003, -0.0087, -0.0264,  0.0185,  0.0030,  0.0051, -0.0001, -0.0016],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.4380, 1.4630, 1.3994, 1.5598, 1.4821, 1.5731, 1.5497, 1.3595, 1.4889,\n",
      "        1.3048, 1.6238, 1.5893, 1.6017, 1.7229, 1.4011, 1.5854, 1.3357, 1.4181,\n",
      "        1.4531, 1.6553, 1.6545, 1.5328, 1.6758, 1.5343, 1.5189, 1.5173, 1.6397,\n",
      "        1.4483, 1.6766, 1.6490, 1.4779, 1.4971, 1.3274, 1.7056, 1.4258, 1.5068,\n",
      "        1.1224, 1.8061, 1.5170, 1.2296, 1.7952, 1.4747, 1.4460, 1.4895, 1.4507,\n",
      "        1.4020, 1.4846, 1.4978, 1.4784, 1.5295, 1.5009, 1.7350, 1.4742, 1.3275,\n",
      "        1.3607, 1.4866, 1.6435, 1.5468, 1.4002, 1.4262, 1.5304, 1.5396, 1.3204,\n",
      "        1.4573], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.1414, -0.1503,  0.0749,  0.0256,  0.0246,  0.2234,  0.0056, -0.0435,\n",
      "        -0.1836, -0.0247,  0.0061, -0.1179, -0.0657, -0.0372, -0.0528,  0.0414,\n",
      "         0.0141,  0.0262,  0.0583,  0.0833, -0.1050, -0.0776,  0.1069, -0.0566,\n",
      "         0.1008,  0.0751,  0.0095, -0.1629,  0.0135, -0.0575,  0.0922,  0.0791,\n",
      "        -0.0464, -0.0276, -0.1484, -0.0542,  0.0276,  0.0324,  0.0092,  0.0289,\n",
      "        -0.1865,  0.0630,  0.0380,  0.1045,  0.1756,  0.0223,  0.0342, -0.1775,\n",
      "         0.0949, -0.0238, -0.0451,  0.0460, -0.0174, -0.0449,  0.0592, -0.0529,\n",
      "        -0.0436, -0.1186, -0.0445,  0.0803,  0.0408,  0.0840, -0.0240,  0.0211],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0980,  0.0546, -0.0588,  ..., -0.0197, -0.0009,  0.0322],\n",
      "        [ 0.0158,  0.0700,  0.0160,  ..., -0.0072, -0.0686, -0.0083],\n",
      "        [ 0.0519, -0.0297, -0.0469,  ...,  0.0576, -0.0441,  0.0607],\n",
      "        ...,\n",
      "        [ 0.0506, -0.0031, -0.0770,  ...,  0.0181, -0.0436,  0.0071],\n",
      "        [ 0.0148,  0.0057,  0.0309,  ...,  0.0010,  0.0848,  0.0018],\n",
      "        [-0.0107, -0.0068, -0.0472,  ...,  0.0519,  0.0047,  0.0302]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.0376, -0.0988, -0.0310,  0.0072, -0.0866, -0.0355,  0.0114, -0.0264,\n",
      "        -0.0764, -0.0485, -0.0544, -0.0412, -0.0305, -0.0661, -0.0956, -0.0464,\n",
      "        -0.0610, -0.0530, -0.0578, -0.0288, -0.0125, -0.0323, -0.0377, -0.0156,\n",
      "        -0.0621, -0.0771, -0.0706, -0.0153, -0.0269, -0.0708, -0.0390, -0.0486,\n",
      "        -0.0857, -0.0481, -0.0489, -0.0744, -0.0691, -0.0142, -0.0060, -0.0183,\n",
      "        -0.0808, -0.0534, -0.0628, -0.0846, -0.0549,  0.0146, -0.0509, -0.0742,\n",
      "        -0.0314, -0.0287, -0.0521, -0.0121, -0.0431, -0.0461, -0.0750,  0.0248,\n",
      "        -0.0274, -0.0838, -0.0584,  0.0096, -0.0514, -0.0107, -0.0495, -0.0632,\n",
      "        -0.0074, -0.0568, -0.0604, -0.0649, -0.0673, -0.0867,  0.0014, -0.0456,\n",
      "        -0.0227, -0.0140, -0.0445, -0.0665, -0.0143, -0.0527, -0.0296, -0.0327,\n",
      "        -0.0625, -0.0591, -0.0612, -0.1097, -0.0581, -0.0800, -0.0223, -0.0910,\n",
      "         0.0168, -0.0553, -0.0470, -0.0755, -0.0306,  0.0024, -0.0113, -0.0531,\n",
      "        -0.0702, -0.0438, -0.0181, -0.0357, -0.0481, -0.0394, -0.0147, -0.0632,\n",
      "        -0.0789, -0.0585, -0.0960,  0.0303, -0.0554, -0.0313, -0.0913, -0.0080,\n",
      "         0.0260, -0.0422, -0.0110, -0.1015, -0.0565, -0.0319, -0.0706, -0.0699,\n",
      "        -0.0632, -0.0641, -0.0247, -0.0834, -0.0427, -0.0055, -0.0416, -0.0338,\n",
      "        -0.0479, -0.0510, -0.0529, -0.0329, -0.0354, -0.0142, -0.0574, -0.0461,\n",
      "        -0.1161, -0.0758, -0.0694, -0.0848, -0.0549, -0.0310, -0.0399, -0.0534,\n",
      "        -0.0487, -0.0553, -0.0574, -0.0698, -0.0170, -0.0577,  0.0121, -0.0193,\n",
      "        -0.0634, -0.0552, -0.0851, -0.0295, -0.0252, -0.0411, -0.0196, -0.0336,\n",
      "        -0.0935, -0.0163, -0.0748, -0.0628, -0.0552, -0.0233, -0.0354,  0.0046,\n",
      "        -0.0789, -0.0085, -0.0190, -0.0389, -0.0390, -0.0109, -0.0342, -0.0402,\n",
      "        -0.0453, -0.0451, -0.0755, -0.0611, -0.0948, -0.0127, -0.0285, -0.0325,\n",
      "        -0.0123, -0.0610, -0.0633, -0.0337, -0.1031, -0.0616, -0.0399, -0.0275,\n",
      "         0.0126, -0.0114, -0.0208, -0.0463, -0.0753, -0.0676, -0.0772, -0.0089,\n",
      "        -0.0692, -0.0640, -0.0943,  0.0111, -0.0346, -0.0130, -0.0347, -0.0436,\n",
      "         0.0075, -0.1120, -0.0252, -0.0030,  0.0445, -0.0573, -0.0351, -0.0298,\n",
      "        -0.0426, -0.0191, -0.0848, -0.0409, -0.0335, -0.0143, -0.0293, -0.0684,\n",
      "        -0.0863,  0.0238, -0.0086, -0.0270,  0.0185, -0.0481, -0.0721, -0.0070,\n",
      "        -0.0022, -0.0999,  0.0201, -0.0388, -0.0345, -0.0080, -0.0040, -0.1050,\n",
      "        -0.0559, -0.0697, -0.0444, -0.0166, -0.0236, -0.0386, -0.0882, -0.0767,\n",
      "        -0.0520, -0.0102, -0.0743, -0.0416, -0.0310, -0.0380, -0.0013, -0.0214],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-5.1154e-03, -8.1059e-03, -5.1012e-02,  ..., -3.5242e-02,\n",
      "         -2.4262e-02, -5.9844e-02],\n",
      "        [ 5.7700e-02, -1.6736e-02,  3.7295e-02,  ..., -1.8933e-03,\n",
      "         -3.4033e-02, -2.4245e-02],\n",
      "        [ 4.9788e-02, -6.3404e-04, -7.5174e-04,  ...,  1.8980e-02,\n",
      "          6.3005e-02,  5.9565e-02],\n",
      "        ...,\n",
      "        [-4.2067e-02,  2.0772e-04, -2.9825e-02,  ..., -4.8290e-02,\n",
      "         -4.7009e-03,  1.7913e-02],\n",
      "        [-2.9395e-02,  7.0894e-03,  1.4568e-02,  ...,  1.2824e-02,\n",
      "         -2.3330e-03, -2.6240e-05],\n",
      "        [ 2.9060e-02, -3.1349e-02, -4.0577e-02,  ..., -7.9427e-04,\n",
      "          6.4520e-02, -8.6331e-02]], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 0.0079, -0.0014, -0.0017,  0.0096,  0.0182,  0.0064,  0.0039, -0.0405,\n",
      "        -0.0300, -0.0298,  0.0031, -0.0004, -0.0035,  0.0050, -0.0045,  0.0012,\n",
      "        -0.0162, -0.0230, -0.0060,  0.0114, -0.0291, -0.0102, -0.0162,  0.0088,\n",
      "         0.0178, -0.0202,  0.0037,  0.0014,  0.0141,  0.0027,  0.0049,  0.0263,\n",
      "        -0.0109, -0.0050, -0.0158,  0.0067,  0.0014, -0.0055, -0.0106,  0.0196,\n",
      "        -0.0082,  0.0073, -0.0058,  0.0090,  0.0323, -0.0235,  0.0036, -0.0090,\n",
      "         0.0242,  0.0096,  0.0003, -0.0060,  0.0108, -0.0269, -0.0268,  0.0058,\n",
      "         0.0116,  0.0132, -0.0049,  0.0181,  0.0003,  0.0007, -0.0077,  0.0112],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.2065, 1.2316, 1.2405, 1.1954, 1.2260, 1.2873, 1.2253, 1.2764, 1.2226,\n",
      "        1.2565, 1.5102, 1.3258, 1.4447, 1.3185, 1.2438, 1.3393, 1.1558, 1.2560,\n",
      "        1.2315, 1.3185, 1.1778, 1.2492, 1.4362, 1.2477, 1.2254, 1.2114, 1.3759,\n",
      "        1.2271, 1.5182, 1.4409, 1.2748, 1.3158, 1.2016, 1.3617, 1.2324, 1.2636,\n",
      "        1.4651, 1.4137, 1.3978, 1.2029, 1.3991, 1.2577, 1.2432, 1.2439, 1.1797,\n",
      "        1.2774, 1.2877, 1.2565, 1.3733, 1.3361, 1.2754, 1.5192, 1.2389, 1.2096,\n",
      "        1.2347, 1.2827, 1.4876, 1.1925, 1.3152, 1.2329, 1.2612, 1.2880, 1.1916,\n",
      "        1.2836], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.1593, -0.0528,  0.1255, -0.0373,  0.2446,  0.2624,  0.0373,  0.0243,\n",
      "        -0.0842, -0.0403, -0.2401, -0.0266,  0.0820,  0.0156, -0.0645,  0.1058,\n",
      "        -0.1032, -0.0537,  0.1197,  0.0259,  0.0128, -0.0235, -0.0957, -0.0701,\n",
      "         0.0297, -0.0291, -0.0584, -0.0674,  0.0356,  0.0853,  0.0900,  0.1464,\n",
      "        -0.1786,  0.0232, -0.1462, -0.0352,  0.2857,  0.0560,  0.0443,  0.0676,\n",
      "        -0.0491,  0.0010,  0.0781,  0.2720,  0.0705, -0.0422,  0.0044, -0.0337,\n",
      "         0.0706, -0.0888,  0.0348,  0.0889, -0.0125, -0.0769, -0.0373,  0.0242,\n",
      "         0.0139, -0.0782, -0.0095,  0.0427,  0.0920, -0.0405, -0.0964,  0.0330],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[-1.7345e-01,  1.9345e-01,  1.1579e-01,  ...,  3.4467e-02,\n",
      "         -2.1712e-02, -1.6165e-01],\n",
      "        [ 4.9284e-02,  2.1731e-01, -1.5649e-02,  ...,  3.3172e-02,\n",
      "          1.2428e-01, -4.3289e-02],\n",
      "        [-1.2714e-01, -1.5143e-01,  1.2867e-01,  ..., -1.0883e-01,\n",
      "          5.7981e-02,  3.8113e-03],\n",
      "        ...,\n",
      "        [ 2.5256e-02,  1.0226e-04, -6.4358e-03,  ...,  4.8356e-02,\n",
      "         -1.3184e-03, -1.8529e-02],\n",
      "        [-1.2963e-03, -2.7055e-03, -8.0382e-03,  ...,  2.7208e-02,\n",
      "          1.3343e-02, -2.8900e-02],\n",
      "        [-2.7364e-03,  1.9550e-03, -2.8629e-02,  ..., -1.1305e-02,\n",
      "          2.3745e-02,  1.6385e-02]], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 3.6413e-01, -2.0676e-01,  2.4988e-02, -3.9063e-01,  4.8867e-01,\n",
      "         3.8905e-01, -3.0166e-01,  3.6855e-01, -2.1082e-01, -9.8554e-02,\n",
      "        -2.9924e-01,  7.0870e-02,  4.3544e-01, -1.2549e-01,  4.6356e-01,\n",
      "        -5.8591e-01, -2.5479e-01,  3.9908e-01, -2.9938e-01,  2.7593e-01,\n",
      "        -4.3049e-02,  2.6178e-01, -2.5676e-01,  7.5151e-02, -3.5164e-02,\n",
      "        -5.4870e-01,  3.8423e-01,  4.2479e-01, -1.1113e-02,  2.7147e-01,\n",
      "         3.8331e-01, -2.8674e-01,  1.4838e-01, -1.7414e-01, -2.2706e-01,\n",
      "         2.4712e-01, -5.2786e-02,  2.2766e-01,  2.9660e-01,  2.2910e-01,\n",
      "         7.4167e-02,  1.4266e-01,  5.2433e-01,  6.3870e-03,  2.9914e-01,\n",
      "        -2.8906e-01,  1.1448e-01,  5.4871e-01, -4.3664e-01, -3.3110e-01,\n",
      "        -3.8233e-01,  2.6999e-01,  1.9595e-01,  3.7282e-01, -2.4235e-01,\n",
      "         2.2960e-01, -4.3991e-02,  1.1665e-01,  3.3671e-01, -2.8389e-01,\n",
      "         4.3096e-01,  8.6089e-02,  3.2823e-01,  5.9214e-01, -2.6588e-01,\n",
      "         1.6900e-01, -1.0733e-01,  8.8659e-02, -6.2497e-03, -2.2833e-01,\n",
      "         1.4100e-01, -2.4465e-01,  2.2075e-01,  1.9154e-01,  2.2769e-01,\n",
      "        -1.3830e-01, -1.4298e-01,  1.4493e-01, -2.7326e-01,  2.4663e-01,\n",
      "         3.2391e-03, -5.4572e-02,  2.1452e-02, -9.5419e-03,  5.1595e-02,\n",
      "         2.8147e-03,  5.8622e-02, -3.6198e-02, -3.1920e-02,  2.1947e-02,\n",
      "        -1.5516e-02, -4.4695e-02,  2.6835e-02, -1.5339e-02, -4.5264e-02,\n",
      "         9.3473e-02, -9.3382e-02, -2.2512e-01,  3.6620e-01, -3.5934e-01,\n",
      "         2.7307e-01, -3.6148e-01, -1.2321e-01, -2.6486e-01, -2.6368e-01,\n",
      "        -3.1901e-01, -1.0865e-01, -1.4321e-01, -4.6831e-02,  2.8817e-01,\n",
      "        -4.1810e-01, -1.9214e-01,  1.6067e-01,  1.6283e-01, -1.0856e-01,\n",
      "         1.9705e-03, -8.8646e-02, -1.2682e-01,  1.3009e-01,  1.6290e-01,\n",
      "         5.3470e-02, -7.1690e-02, -1.4108e-01,  1.1650e-01, -7.3589e-02,\n",
      "        -1.0394e-01, -1.5047e-01, -1.4961e-01, -3.4589e-03,  9.7279e-03,\n",
      "         9.4746e-03, -2.5966e-02, -3.1488e-02, -4.7426e-03,  2.7385e-02,\n",
      "         4.9993e-02,  1.4015e-02,  5.0314e-03,  3.0687e-02, -4.0652e-02,\n",
      "        -2.9192e-02,  2.7023e-02,  1.8108e-02, -5.0082e-03, -8.6914e-03,\n",
      "         2.3704e-02, -2.6858e-02, -3.3402e-02, -2.0590e-03,  4.5640e-02,\n",
      "        -5.7703e-03, -4.6077e-02,  6.7510e-03,  1.5786e-02,  2.9194e-02,\n",
      "         3.2160e-02, -5.7802e-03, -2.4392e-02, -3.5801e-02, -3.1955e-02,\n",
      "         9.8508e-04,  3.4321e-02, -8.9049e-03, -3.5705e-02, -2.9672e-02,\n",
      "        -2.2352e-02,  1.0756e-02, -4.5222e-02,  3.1734e-02, -8.5496e-03,\n",
      "        -1.8833e-02,  1.8867e-02,  3.0464e-02, -2.0890e-02, -1.0078e-02,\n",
      "        -4.1275e-02, -8.4965e-03,  9.6535e-04,  1.1619e-02,  1.5988e-02,\n",
      "        -8.3039e-03, -7.4041e-03, -4.1048e-04, -2.3551e-02, -6.4582e-03,\n",
      "        -1.3946e-02, -3.4693e-02, -2.5890e-02,  3.3175e-03,  2.9931e-02,\n",
      "        -2.3109e-02,  6.1198e-03], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0676, -0.0310, -0.0045,  ...,  0.0454, -0.0558,  0.0557],\n",
      "        [-0.0392, -0.0569,  0.0487,  ...,  0.0289, -0.0011, -0.0283],\n",
      "        [-0.1177,  0.0265, -0.0192,  ..., -0.0054,  0.0468, -0.0033],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0133,  0.0558,  ...,  0.0503,  0.0174, -0.0163],\n",
      "        [-0.0228,  0.0614,  0.0350,  ...,  0.0467, -0.0257, -0.0336],\n",
      "        [-0.0252,  0.0047, -0.0147,  ...,  0.0071,  0.0533,  0.0216]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([ 0.0016,  0.0079,  0.0081,  0.0012,  0.0210, -0.0100,  0.0107, -0.0046,\n",
      "        -0.0010, -0.0030, -0.0049, -0.0055, -0.0041, -0.0077,  0.0087, -0.0105,\n",
      "        -0.0184, -0.0057,  0.0119,  0.0124, -0.0032, -0.0022,  0.0150,  0.0125,\n",
      "         0.0045, -0.0124, -0.0011, -0.0087,  0.0078, -0.0114, -0.0053,  0.0051,\n",
      "        -0.0013,  0.0019,  0.0100,  0.0069, -0.0272, -0.0115, -0.0132,  0.0268,\n",
      "         0.0026,  0.0109, -0.0049,  0.0053, -0.0022,  0.0081,  0.0109,  0.0056,\n",
      "        -0.0048,  0.0083,  0.0050, -0.0026, -0.0148, -0.0060, -0.0181,  0.0159,\n",
      "        -0.0043,  0.0076,  0.0095, -0.0129, -0.0051, -0.0159,  0.0088, -0.0164],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([1.5809, 1.4204, 1.4551, 1.7347, 1.3854, 1.6341, 1.6501, 1.5553, 1.5374,\n",
      "        1.5204, 1.8779, 1.7174, 1.7371, 1.8501, 1.5343, 1.5885, 1.4279, 1.5035,\n",
      "        1.5225, 1.9399, 1.7530, 1.6672, 1.7408, 1.6330, 1.5772, 1.6120, 1.7199,\n",
      "        1.6182, 2.0304, 1.6619, 1.5321, 1.5646, 1.5574, 1.8549, 1.4104, 1.7211,\n",
      "        1.1023, 1.8980, 1.6264, 1.3942, 1.8111, 1.5431, 1.5634, 1.4910, 1.5879,\n",
      "        1.5538, 1.6444, 1.5086, 1.6206, 1.5930, 1.5800, 1.8757, 1.6278, 1.4474,\n",
      "        1.5648, 1.6550, 1.9197, 1.5597, 1.5863, 1.6039, 1.7125, 1.6927, 1.5119,\n",
      "        1.5375], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.1318,  0.1134,  0.0768, -0.0962,  0.0498,  0.0720, -0.1092,  0.0432,\n",
      "         0.0156, -0.0011, -0.0402, -0.0307,  0.0331,  0.0248,  0.0354, -0.0994,\n",
      "         0.0034,  0.0319,  0.0902, -0.0376,  0.0378,  0.0090,  0.0399, -0.0895,\n",
      "        -0.0607, -0.0110,  0.0278, -0.0077, -0.0608,  0.0136, -0.0076,  0.0907,\n",
      "         0.0494, -0.0127,  0.0536, -0.0875,  0.2377, -0.0452, -0.0915, -0.0322,\n",
      "         0.0131,  0.1086, -0.1428,  0.0294, -0.0418, -0.0983,  0.0663, -0.1965,\n",
      "         0.0680, -0.0115,  0.0200,  0.0193,  0.0792,  0.0234,  0.0922, -0.0812,\n",
      "         0.0144, -0.0883, -0.0351,  0.0390, -0.0513,  0.0624,  0.0407, -0.0940],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0739,  0.0224, -0.0705,  ...,  0.0109,  0.0314,  0.0013],\n",
      "        [ 0.0431,  0.0260, -0.0341,  ...,  0.0997,  0.0009,  0.0329],\n",
      "        [ 0.0925, -0.0379,  0.0012,  ...,  0.1030, -0.0066, -0.0588],\n",
      "        ...,\n",
      "        [-0.0089, -0.0748,  0.0219,  ..., -0.0046, -0.0776, -0.0036],\n",
      "        [ 0.0025,  0.1055,  0.0394,  ..., -0.0799,  0.0475, -0.0370],\n",
      "        [ 0.0278,  0.0028,  0.0271,  ...,  0.0091,  0.0324,  0.0976]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-9.2139e-02,  2.3230e-02, -5.3486e-02, -7.9067e-02, -4.0444e-03,\n",
      "        -4.8159e-02,  2.6666e-02, -1.7905e-02, -9.9141e-02, -3.0600e-02,\n",
      "        -3.0894e-02,  3.4226e-02, -9.1208e-03,  6.5894e-04, -4.7297e-02,\n",
      "        -2.8349e-02,  6.6492e-02, -2.9862e-02, -4.9149e-02, -8.5067e-03,\n",
      "        -1.8996e-02, -5.8841e-02, -2.1190e-02, -5.9145e-03, -1.8692e-01,\n",
      "        -7.6208e-02,  7.0890e-04, -1.9094e-02, -5.3338e-02, -1.1197e-01,\n",
      "        -1.2545e-01,  2.6035e-06, -9.2529e-02, -1.1421e-01, -9.7933e-02,\n",
      "        -1.0994e-02, -5.6499e-03,  3.1925e-02, -9.3745e-03, -5.5211e-04,\n",
      "        -4.4300e-02, -1.3647e-01,  1.1916e-02, -1.2177e-02,  7.4973e-02,\n",
      "         7.0218e-02, -7.4851e-02, -3.8960e-02,  5.5832e-02, -4.0104e-02,\n",
      "         2.8125e-02, -1.0696e-03, -7.3121e-02, -6.0239e-03, -8.1232e-03,\n",
      "         1.7187e-02, -7.4612e-02, -7.9250e-02, -7.1943e-02, -1.4479e-03,\n",
      "         2.7401e-02, -7.1467e-02,  3.9904e-02, -1.0394e-02, -5.9261e-02,\n",
      "        -3.8000e-02,  9.1415e-03, -8.3937e-02, -1.4551e-01, -1.1423e-01,\n",
      "         3.3930e-02, -7.5103e-02, -4.2621e-03,  5.9469e-02, -6.2526e-02,\n",
      "        -3.3081e-02, -3.7763e-02, -8.2031e-02, -5.6442e-03, -1.6116e-02,\n",
      "        -7.5690e-02, -5.6736e-02, -3.9510e-02, -8.1467e-02, -8.4565e-02,\n",
      "        -1.2154e-01, -4.8044e-02, -5.5611e-03, -5.6299e-02,  1.4181e-02,\n",
      "         6.8328e-03, -1.4385e-02, -7.3088e-02, -5.4678e-02,  4.5698e-02,\n",
      "        -5.2119e-02,  1.1377e-02, -1.3768e-01,  1.4460e-02, -5.9646e-02,\n",
      "        -4.9282e-02, -3.3749e-02, -4.4929e-02, -2.8769e-02, -3.9729e-03,\n",
      "        -3.4372e-02, -8.2623e-02,  4.3154e-02, -5.2384e-02, -5.7813e-02,\n",
      "         6.2361e-02, -4.3486e-02, -2.2935e-02, -3.2380e-02, -1.7580e-02,\n",
      "         5.6119e-02, -2.2769e-02, -1.1542e-01, -4.5556e-02, -5.9025e-02,\n",
      "         6.7535e-03, -4.2513e-05, -1.1079e-02, -4.6962e-02, -6.1548e-02,\n",
      "        -5.1555e-02, -4.5524e-02, -1.7065e-02, -8.0636e-02, -5.6807e-02,\n",
      "         2.1580e-02,  5.8954e-02, -4.5544e-02, -2.5530e-02,  2.0760e-02,\n",
      "        -2.9847e-02, -7.6983e-04, -5.8586e-02,  1.7054e-02, -2.1287e-02,\n",
      "        -4.1251e-03, -1.8518e-02, -6.5109e-02, -8.7416e-02, -9.2140e-02,\n",
      "         1.4620e-02, -6.2977e-02,  2.0204e-02, -6.6436e-02, -3.7344e-02,\n",
      "         5.6348e-02,  4.5601e-02, -8.9571e-02, -4.8605e-02,  1.1173e-02,\n",
      "         7.6866e-02, -2.0875e-02, -8.6832e-02, -4.0698e-02, -3.6723e-02,\n",
      "        -1.3109e-01, -7.7504e-02,  1.2685e-02, -1.0807e-01, -5.4984e-02,\n",
      "        -8.4329e-02,  3.8818e-02,  4.6099e-02, -2.8864e-02, -5.9938e-02,\n",
      "        -1.0860e-01, -3.7476e-02,  8.4828e-03, -6.4459e-02, -5.6596e-02,\n",
      "        -1.4713e-02,  2.1219e-03, -1.1527e-01, -4.5094e-03, -5.2618e-02,\n",
      "        -1.5566e-02,  1.5205e-02,  1.6102e-02, -9.7736e-04, -5.6487e-02,\n",
      "         5.7627e-02, -2.1140e-03,  4.7445e-02, -1.0970e-01, -7.5993e-02,\n",
      "        -8.4412e-03, -6.1655e-02,  1.9900e-02, -1.0467e-01,  1.4060e-02,\n",
      "        -5.8938e-02, -3.7249e-02,  3.2004e-02, -3.3202e-02,  3.8468e-02,\n",
      "        -7.5922e-02, -7.0315e-03, -2.5144e-02, -5.9014e-02, -1.6641e-02,\n",
      "        -1.4410e-02, -1.0868e-01,  9.3824e-03, -8.4958e-02, -8.4471e-02,\n",
      "        -6.9843e-02, -5.4143e-02,  1.4322e-02, -1.0670e-01, -5.8435e-02,\n",
      "        -5.9389e-02, -4.4800e-02, -5.6712e-02, -7.7459e-02, -4.0808e-02,\n",
      "        -5.2114e-02, -9.0593e-02,  4.9248e-02, -5.7999e-02, -7.9817e-02,\n",
      "        -1.2919e-01, -1.1649e-01, -2.6080e-02,  4.3983e-02, -2.2156e-02,\n",
      "        -6.4824e-02, -3.1594e-02, -9.7307e-04, -1.5610e-02, -6.8559e-02,\n",
      "        -3.8630e-02, -7.4078e-02, -9.7522e-02, -5.0581e-02,  5.1859e-04,\n",
      "        -8.7047e-02, -2.5186e-02, -4.0530e-02, -4.4370e-02,  7.6993e-02,\n",
      "        -2.4181e-02,  4.2198e-02, -2.5441e-02, -7.3325e-02, -1.2625e-02,\n",
      "        -9.0722e-02, -8.6028e-02, -5.3029e-02, -7.8668e-02, -6.4158e-02,\n",
      "        -5.6269e-02], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([[ 0.0446, -0.0512, -0.0478,  ...,  0.0035, -0.0175,  0.0041],\n",
      "        [ 0.0820, -0.0197,  0.0934,  ..., -0.0231, -0.0169, -0.0226],\n",
      "        [ 0.0206, -0.0004,  0.0359,  ...,  0.0478, -0.0107, -0.0072],\n",
      "        ...,\n",
      "        [-0.0041, -0.0127, -0.0007,  ..., -0.0671, -0.0101,  0.0216],\n",
      "        [-0.0140, -0.0080,  0.0170,  ..., -0.0175,  0.0195, -0.0296],\n",
      "        [ 0.0356,  0.0972,  0.0155,  ..., -0.0328, -0.0661, -0.0169]],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-0.0073, -0.0092, -0.0078,  0.0077,  0.0231, -0.0174,  0.0329, -0.0335,\n",
      "        -0.0423, -0.0022,  0.0154,  0.0601,  0.0314, -0.0462, -0.0299,  0.0107,\n",
      "         0.0008,  0.0071, -0.0138,  0.0279, -0.0347, -0.0479, -0.0042,  0.0065,\n",
      "         0.0366, -0.0135, -0.0174,  0.0123,  0.0169,  0.0203,  0.0072,  0.0221,\n",
      "        -0.0014, -0.0106, -0.0098,  0.0600, -0.0883, -0.0295,  0.0147,  0.0149,\n",
      "         0.0362, -0.0027,  0.0305,  0.0383,  0.0178, -0.0158,  0.0078, -0.0014,\n",
      "        -0.0042, -0.0146, -0.0046,  0.0105, -0.0276, -0.0446, -0.0183,  0.0115,\n",
      "         0.0330, -0.0103, -0.0301,  0.0223,  0.0198, -0.0146, -0.0003,  0.0190],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([4.8168, 4.8013, 4.7399, 4.6493, 4.7488, 4.4793, 4.6465, 4.8564, 4.7148,\n",
      "        4.8260, 4.2878, 4.7392, 4.5913, 4.4966, 4.6805, 4.8044, 4.7525, 4.8731,\n",
      "        4.7980, 4.4671, 4.1772, 4.6634, 4.6311, 4.8591, 3.9715, 4.4661, 4.6087,\n",
      "        4.8321, 4.4528, 4.7019, 4.7140, 4.8194, 4.7579, 4.6737, 4.6623, 4.7392,\n",
      "        3.3713, 4.4554, 4.5193, 4.6411, 4.3481, 4.8051, 4.8504, 4.8578, 4.8357,\n",
      "        4.8175, 4.7734, 4.7106, 4.7472, 4.7660, 4.8244, 4.4627, 4.6728, 4.6214,\n",
      "        4.6225, 4.6766, 4.3636, 4.5608, 4.5152, 4.7541, 4.7684, 4.6970, 4.8876,\n",
      "        4.7975], requires_grad=True) not found in saved optimizer state.\n",
      "Parameter Parameter containing:\n",
      "tensor([-1.3721e+00, -5.4727e-01,  1.3403e+00,  4.9486e-01,  1.8239e+00,\n",
      "         1.2510e+00,  7.0091e-01, -7.5851e-01, -1.3076e+00, -8.5631e-02,\n",
      "        -1.1227e+00,  8.4131e-01,  1.1313e+00, -3.0740e-01, -1.2604e+00,\n",
      "         9.0809e-01, -7.5030e-01, -8.7539e-01,  1.5935e+00,  4.4644e-01,\n",
      "        -8.0027e-01, -1.4889e+00, -8.5036e-01,  5.1654e-01,  1.0255e+00,\n",
      "        -7.8598e-01, -1.2563e+00,  7.8595e-01,  5.1910e-01,  1.3196e+00,\n",
      "         1.5913e+00,  8.3160e-01, -1.2341e+00, -5.3756e-01, -3.9872e-01,\n",
      "         1.0157e+00, -4.1866e-04, -1.2522e+00,  8.4238e-01,  1.4156e+00,\n",
      "        -5.0544e-01,  1.1105e+00,  5.7772e-01,  3.3515e-01,  7.5350e-01,\n",
      "        -1.0007e+00,  1.0759e+00, -5.2535e-02,  1.1437e+00, -1.3612e+00,\n",
      "        -7.2656e-01,  1.3648e+00, -9.3327e-01, -1.2084e+00, -5.2529e-01,\n",
      "         3.6602e-01,  9.7883e-01, -3.4556e-01, -1.4324e+00,  1.5905e+00,\n",
      "         8.0672e-01, -1.3855e+00, -8.9409e-01,  5.5984e-01],\n",
      "       requires_grad=True) not found in saved optimizer state.\n",
      "Model and optimizer state loaded successfully.\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(24899, 64)\n",
      "    (wpe): Embedding(64, 64)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=64, out_features=192, bias=True)\n",
      "          (c_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=24899, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Define the GPT model and configuration\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int  # n_embd must be divisible by n_head\n",
    "    dropout: float\n",
    "    bias: bool\n",
    "\n",
    "# Load the saved configuration details\n",
    "save_dir = r'C:\\Users\\ajazb\\Desktop\\AI training\\GPTper_saves'\n",
    "if not os.path.exists(save_dir):\n",
    "    raise Exception(f\"Save directory {save_dir} does not exist!\")\n",
    "\n",
    "# Read the configuration details if needed (e.g., epochs, gradient accumulation steps)\n",
    "config_path = os.path.join(save_dir, 'training_config_perchin.txt')\n",
    "with open(config_path, 'r') as f:\n",
    "    config_details = f.read()\n",
    "    print(config_details)  # For debugging\n",
    "\n",
    "# Define the configuration, ensure the values match your saved configuration\n",
    "config = GPTConfig(\n",
    "    block_size=64,\n",
    "    vocab_size=24899,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=64,  # Ensure n_embd is divisible by n_head\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT(config)\n",
    "\n",
    "# Load the saved model state with map_location to map CUDA tensors to the CPU\n",
    "model_path = os.path.join(save_dir, 'final_model_perchin.pt')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Initialize the optimizer with the model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Load the saved optimizer state\n",
    "optimizer_path = os.path.join(save_dir, 'final_optimizer_perchin.pt')\n",
    "saved_optimizer_state = torch.load(optimizer_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Debug prints to compare parameter groups\n",
    "print(f\"Current optimizer param_groups: {len(optimizer.param_groups)}\")\n",
    "print(f\"Saved optimizer param_groups: {len(saved_optimizer_state['param_groups'])}\")\n",
    "\n",
    "# Check if the parameter groups match\n",
    "if len(saved_optimizer_state['param_groups']) != len(optimizer.param_groups):\n",
    "    print(\"Parameter groups do not match.\")\n",
    "    # Optionally handle this mismatch, e.g., adjust the model or optimizer definition\n",
    "\n",
    "# Initialize the optimizer with the model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Load the saved optimizer state\n",
    "optimizer_path = os.path.join(save_dir, 'final_optimizer_perchin.pt')\n",
    "saved_optimizer_state = torch.load(optimizer_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the optimizer state manually\n",
    "for param_group in optimizer.param_groups:\n",
    "    for p in param_group['params']:\n",
    "        state = optimizer.state[p]\n",
    "        if p in saved_optimizer_state['state']:\n",
    "            state.update(saved_optimizer_state['state'][p])\n",
    "        else:\n",
    "            print(f\"Parameter {p} not found in saved optimizer state.\")\n",
    "\n",
    "print(\"Model and optimizer state loaded successfully.\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8db637-025f-425e-afc8-27263b1f4260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text:\n",
      "       [BOM]\n",
      "       [EOS] [BOM]\n",
      "      [BOM]\n",
      "      [EOS] [BOM]\n",
      "        [BOM]\n",
      "        [EOS] [BOM]\n",
      "          [BOM]\n",
      "      [EOS] [BOM]\n",
      "         [BOM]\n",
      "          [EOS] [BOM]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a Persian-specific tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "\n",
    "\n",
    "# Function to tokenize input text and truncate to block size\n",
    "def tokenize_and_truncate_text(text, block_size):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > block_size:\n",
    "        tokens = tokens[-block_size:]  # Truncate to the last block_size tokens\n",
    "    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to decode output tokens\n",
    "def decode_output(predicted_ids):\n",
    "    predicted_tokens = predicted_ids.tolist()  # Convert tensor to list\n",
    "    predicted_text = tokenizer.decode(predicted_tokens)\n",
    "    return predicted_text\n",
    "\n",
    "# Function to get model output for a single input text\n",
    "def get_model_output(model, input_text, config, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Tokenize and truncate input text\n",
    "    input_ids = tokenize_and_truncate_text(input_text, config.block_size)\n",
    "    input_ids = input_ids.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(input_ids)\n",
    "        logits = logits[:, -1, :] / temperature  # Apply temperature\n",
    "\n",
    "        # Top-k sampling\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        chosen_idx = torch.multinomial(probs, num_samples=1)\n",
    "        predicted_token_id = top_k_indices[0, chosen_idx[0]]\n",
    "\n",
    "        # Decode the predicted token\n",
    "        predicted_text = decode_output(predicted_token_id)\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "# Function to validate the predicted text\n",
    "def is_valid_token(token):\n",
    "    # Implement your logic to check if the token is valid or not\n",
    "    # This can include checking for known invalid sequences, punctuation, etc.\n",
    "    invalid_sequences = [' ']\n",
    "    return token not in invalid_sequences\n",
    "\n",
    "# Sample input text\n",
    "\n",
    "input_text = \"     \"\n",
    "input_text = \"   \"\n",
    "input_text = \"   \"\n",
    "input_text = \"     \"\n",
    "input_text = \"    \"\n",
    "input_text = \"     \"\n",
    "input_text = \"     \"\n",
    "input_text = \"   \"\n",
    "input_text = \"     \"\n",
    "input_text = \"     \"\n",
    "\n",
    "\n",
    "# Define maximum BOM count\n",
    "max_bom_count = 10\n",
    "\n",
    "# Accumulate the complete generated text\n",
    "complete_text = input_text\n",
    "\n",
    "# Track the last few generated tokens\n",
    "repetition_threshold = 5\n",
    "last_few_tokens = []\n",
    "temperature = 0.7\n",
    "\n",
    "# Generate text in a loop until the maximum BOM count is reached\n",
    "bom_count = 0\n",
    "\n",
    "while bom_count < max_bom_count:\n",
    "    predicted_text = get_model_output(model, input_text, config, temperature=0.7, top_k=50)\n",
    "\n",
    "    # Prevent repeating sequences and filter invalid tokens\n",
    "    if predicted_text.strip() in last_few_tokens or not is_valid_token(predicted_text.strip()):\n",
    "        temperature *= 1.1  # Increase temperature to promote diversity\n",
    "        predicted_text = get_model_output(model, input_text, config, temperature=temperature, top_k=50)\n",
    "    else:\n",
    "        temperature = max(0.7, temperature * 0.95)  # Gradually decrease temperature back to a minimum\n",
    "\n",
    "    complete_text += ' ' + predicted_text.strip()  # Ensure proper spacing\n",
    "    last_few_tokens.append(predicted_text.strip())\n",
    "\n",
    "    if len(last_few_tokens) > repetition_threshold:\n",
    "        last_few_tokens.pop(0)\n",
    "\n",
    "    # Truncate the input text if it exceeds the block size\n",
    "    input_text += ' ' + predicted_text.strip()\n",
    "    input_ids = tokenize_and_truncate_text(input_text, config.block_size)\n",
    "    input_text = tokenizer.decode(input_ids[0].tolist())  # Update input_text to truncated version\n",
    "\n",
    "    # Add a new line after [EOS]\n",
    "    if '[EOS]' and '[BOM]' in predicted_text:\n",
    "        complete_text += '\\n'\n",
    "        bom_count += 1\n",
    "\n",
    "# Print the whole output after the loop\n",
    "print(f\"Predicted Text:\\n{complete_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb880f-fe74-44d0-8e8c-3e37035e422d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ef1da-f7e5-4eee-9901-e7d6b6073bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
